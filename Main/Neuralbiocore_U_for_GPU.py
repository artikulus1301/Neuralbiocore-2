import torch

print(torch.__version__)
print(torch.cuda.is_available())
print(torch.version.cuda)

if torch.cuda.is_available():
    print("GPU –Ω–∞–π–¥–µ–Ω!")
    print("–ò–º—è:", torch.cuda.get_device_name(0))
    print("CUDA –≤–µ—Ä—Å–∏—è:", torch.version.cuda)
    print("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU:", torch.cuda.device_count())
else:
    print("GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

import math
import os
import numpy as np
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from enum import Enum
import matplotlib.pyplot as plt
from tqdm import tqdm
import zlib
import json

"""
NeuralBiocore_U.py 
–ù–µ –ø—Ä–æ—Å—Ç–æ in-silico –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ú–æ–¥–µ–ª–∏ –°–æ–∑–Ω–∞–Ω–∏—è –Ω–æ –∏ –±—É–¥—É—â–∏–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –Ω–µ–π—Ä–æ–º–æ—Ä—Ñ–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.
"""

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using Device: {DEVICE}")

# torch.set_default_dtype(torch.float64) # REMOVED: Optimize memory
torch.set_float32_matmul_precision('high') # Enable TF32 for Speed

# ==========================================
# 0. JIT KERNELS (PyTorch 2.0+)
# ==========================================

#torch.compile(mode="reduce-overhead")
def dynamics_kernel(V, phase, spikes, refractory_timer, ATP, is_dead,
                   I_drive_total, 
                   s_gaba_a, s_gaba_b,
                   gaba_concentration,
                   natural_omegas,
                   chem_dopamine, 
                   dt, tau_mem, v_rest, v_threshold, refractory_period,
                   c_spike, c_recovery, c_baseline, critical_atp,
                   tau_gaba_a, tau_gaba_b, g_gaba_a_mod,
                   coupling_strength, alpha_sync, general_suppression,
                   tonic_inhibition):
    
    # 1. Energy Dynamics
    live_mask = ~is_dead
    can_spike = (ATP > c_spike) & live_mask
    
    # In-place updates for ATP
    # ATP.add_(c_recovery - c_baseline) # Global update (masked later?)
    # Logic: ATP = ATP + c_recovery - c_baseline if live
    
    # Calculate delta for live neurons
    delta_atp = torch.zeros_like(ATP)
    delta_atp.masked_fill_(live_mask, c_recovery - c_baseline)
    ATP.add_(delta_atp)
    
    # Subtract cost of spikes
    # ATP = ATP - spikes.float() * c_spike
    ATP.sub_(spikes.float() * c_spike)
    
    dying_now = (ATP < critical_atp) & live_mask
    # Apply death effects
    is_dead.logical_or_(dying_now)
    
    # V = torch.where(dying_now, 0.2, V)
    V.masked_fill_(dying_now, 0.2)
    
    # spikes = torch.where(dying_now, False, spikes)
    spikes.masked_fill_(dying_now, False)
    
    # phase = torch.where(dying_now, 0.0, phase)
    phase.masked_fill_(dying_now, 0.0)
    
    live_mask = ~is_dead
    
    ATP.clamp_(0.0, 1.0)
    
    # 2. GABA Dynamics
    decay_a = (1.0 - dt / tau_gaba_a)
    growth_a = dt * 10.0 * gaba_concentration
    s_gaba_a.mul_(decay_a).add_(growth_a * (1.0 - s_gaba_a)).clamp_(0.0, 1.0)
    
    decay_b = (1.0 - dt / tau_gaba_b)
    growth_b = dt * 5.0 * gaba_concentration
    s_gaba_b.mul_(decay_b).add_(growth_b * (1.0 - s_gaba_b)).clamp_(0.0, 1.0)
    
    # 3. Neuron Dynamics (LIF) - In-place accumulation
    dv = I_drive_total - tonic_inhibition
    dv.sub_(V)
    
    # GABA Effect
    total_conductance = (s_gaba_a * g_gaba_a_mod).add_(s_gaba_b * 0.5)
    dv.sub_(total_conductance * (V + 0.7))
    dv.mul_(dt / tau_mem)
    
    not_refractory = (refractory_timer <= 0)
    update_mask = not_refractory & live_mask
    
    # V += dv where update_mask
    V.masked_scatter_(update_mask, V[update_mask] + dv[update_mask])
    V.clamp_(-2.0, 5.0)
    
    # refractory_timer -= dt
    refractory_timer.sub_(dt).clamp_(min=0.0)
    
    # 4. Phase Dynamics
    complex_phases = torch.exp(1j * phase)
    z = torch.mean(complex_phases)
    R = torch.abs(z)
    Psi = torch.angle(z)
    
    effective_coupling_val = coupling_strength * general_suppression
    freq_mod = 1.0 / (1.0 + 0.3 * (s_gaba_a + s_gaba_b))
    omega_eff = natural_omegas * freq_mod * (1 + 0.15 * chem_dopamine)
    
    # dphi
    dphi = torch.sin(Psi - phase).mul_(effective_coupling_val * R)
    dphi.add_(omega_eff).add_(V * 0.02).mul_(dt)
    
    # Update phase only for live neurons
    phase_update = torch.remainder(phase + dphi, 2 * math.pi)
    phase.masked_scatter_(live_mask, phase_update[live_mask])

    # 5. Spiking
    base_threshold = v_threshold
    adaptive_threshold = base_threshold * (1.0 + 0.2 * s_gaba_a)
    phase_mod = 1.0 + alpha_sync * torch.cos(phase)
    effective_threshold = adaptive_threshold / phase_mod
    
    new_spikes = (V > effective_threshold) & not_refractory & can_spike
    
    # Reset
    V.masked_fill_(new_spikes, v_rest)
    refractory_timer.masked_fill_(new_spikes, refractory_period)
    
    return V, phase, spikes, new_spikes, refractory_timer, ATP, is_dead, s_gaba_a, s_gaba_b

# @torch.compile(fullgraph=False) # DISABLED: Requires MSVC/C++ compiler on Windows
def fused_layer_step(
    V, phase, spikes, refractory_timer, ATP, is_dead,
    s_gaba_a, s_gaba_b,
    V_PV, V_SST, V_VIP, 
    I_exc, I_ext, I_mirror,
    chem_dopamine, chem_acetylcholine, propofol_conc,
    global_context,
    dt, 
    tau_mem, v_rest, v_threshold, refractory_period,
    c_spike, c_recovery, c_baseline, critical_atp,
    tau_gaba_a, tau_gaba_b, g_gaba_a_mod,
    coupling_strength, alpha_sync,
    natural_omegas,
    initial_atp 
):
    # 1. Chemistry & Modulation
    p = propofol_conc
    general_suppression = torch.exp(-p / 1.9)
    paradoxical_boost = 0.8 * torch.exp(-((p - 2.5) ** 2) / 0.5)
    drive_modulator = torch.clamp(general_suppression + paradoxical_boost, 0.01, 2.3)
    interneuron_suppression = 1.0 / (1.0 + 1.5 * paradoxical_boost)

    mean_spikes = spikes.float().mean()

    # 2. Interneurons (Inlined)
    # PV
    V_PV.add_(dt * (-V_PV + mean_spikes * 15.0 * interneuron_suppression))
    spikes_pv = V_PV > 1.0
    V_PV.masked_fill_(spikes_pv, 0.0)
    
    # SST
    inhibition_from_vip = (V_VIP > 0.8).float().mean() * 5.0
    V_SST.add_(dt * (-V_SST + mean_spikes * 8.0 * interneuron_suppression - inhibition_from_vip))
    spikes_sst = V_SST > 1.0
    V_SST.masked_fill_(spikes_sst, 0.0)
    
    # VIP
    V_VIP.add_(dt * (-V_VIP + 0.15) + torch.randn_like(V_VIP) * 0.1 * math.sqrt(dt))

    # GABA Concentration
    pv_act = spikes_pv.float().mean()
    sst_act = spikes_sst.float().mean()
    raw_conc = (pv_act * 1.2 + sst_act) * 8.0 
    gaba_concentration = torch.clamp(raw_conc, max=6.0)

    # 3. Input Accumulation (Fused)
    I_total = I_exc + I_ext + I_mirror 
    
    # Background
    I_background = 2.0 * chem_acetylcholine + 0.5
    I_total.add_(I_background)
    
    # Global Context
    if global_context is not None:
        if global_context.numel() > 1:
             if global_context.shape[0] == V.shape[0]:
                I_total.add_(global_context * 2.0)
             else:
                I_total.add_(global_context.mean() * 2.0)
        else:
             I_total.add_(global_context * 2.0)
             
    # Noise
    sqrt_dt = math.sqrt(dt)
    noise = torch.randn_like(V) * (1.2 * drive_modulator * sqrt_dt)
    I_total.add_(noise)
    
    # Modulation
    I_total.mul_(drive_modulator)

    # 4. Call Kernel
    (
        V, phase, spikes, new_spikes, refractory_timer, ATP, is_dead, s_gaba_a, s_gaba_b
    ) = dynamics_kernel(
        V, phase, spikes, refractory_timer, ATP, is_dead,
        I_total, 
        s_gaba_a, s_gaba_b,
        gaba_concentration,
        natural_omegas,
        chem_dopamine, 
        dt, 
        tau_mem, v_rest, v_threshold, refractory_period,
        c_spike, c_recovery, c_baseline, critical_atp,
        tau_gaba_a, tau_gaba_b, g_gaba_a_mod,
        coupling_strength, alpha_sync, general_suppression,
        propofol_conc * 2.0 
    )
    
    return V, phase, spikes, new_spikes, refractory_timer, ATP, is_dead, s_gaba_a, s_gaba_b, V_PV, V_SST, V_VIP

# ==========================================
# 1. –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –ö–û–ù–°–¢–ê–ù–¢–´
# ==========================================

@dataclass
class TopologyConfig:
    """Small-World network topology configuration"""
    topology_type: str = "small_world"
    k_local: int = 20
    p_rewire: float = 0.08
    shortcut_density: float = 0.03
    bottom_up_locality: int = 15
    top_down_locality: int = 35
    local_boost: float = 2.031
    enable_modules: bool = True
    n_modules: int = 4
    inter_module_sparsity: float = 0.85

@dataclass
class PhysicsConfig:
    """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–∑–∏–∫–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏"""
    dt: float = 0.001
    N_neurons: int = 1000
    
    # –ú–µ–º–±—Ä–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    tau_mem: float = 0.05
    v_rest: float = 0.0
    v_threshold: float = 1.1
    refractory_period: float = 0.005
    
    # –§–∞–∑–æ–≤–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ (–ì–∞–º–º–∞-—Ä–∏—Ç–º ~40–ì—Ü)
    omega_base: float = 2 * math.pi * 40.0
    omega_std: float = 2 * math.pi * 0.5
    coupling_strength: float = 0.5
    alpha_sync: float = 0.3
    
    # –≠–Ω–µ—Ä–≥–∏—è –∏ –°–º–µ—Ä—Ç—å
    initial_atp: float = 1.0
    c_spike: float = 0.001
    c_synapse: float = 0.001
    c_baseline: float = 0.0001
    c_recovery: float = 0.002
    critical_atp: float = 0.01

@dataclass
class NeurogenesisConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –º–æ–∑–≥–∞.
    """
    # –ü—Ä–µ–¥–µ–ª—ã (OOM Protection)
    max_neurons_per_layer: int = 10000  # Verified Safe Limit (80% of OOM crash at 12k)
    
    # –§–∞–∑–∞ 1: –ë—ã—Å—Ç—Ä—ã–π —Ä–æ—Å—Ç (Morphogenesis)
    initial_neurons: int = 100         # –° —á–µ–≥–æ –Ω–∞—á–∏–Ω–∞–µ–º (—ç–º–±—Ä–∏–æ–Ω)
    base_target_neurons: int = 8000    # –¶–µ–ª—å "—Å–æ–∑—Ä–µ–≤–∞–Ω–∏—è"
    growth_rate_fast: int = 50         # –ù–µ–π—Ä–æ–Ω–æ–≤ –∑–∞ —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    
    # –§–∞–∑–∞ 2: –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Ä–æ—Å—Ç (Adult Plasticity)
    growth_rate_slow: int = 5          # –ù–µ–π—Ä–æ–Ω–æ–≤ –ø—Ä–∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    error_threshold_trigger: float = 0.5 # –£—Ä–æ–≤–µ–Ω—å Free Energy –¥–ª—è —Ç—Ä–∏–≥–≥–µ—Ä–∞ —Ä–æ—Å—Ç–∞ (—Å —É—á–µ—Ç–æ–º clipping)
    atp_cost_per_neuron: float = 0.8   # –¢—Ä–µ–±—É–µ–º—ã–π —É—Ä–æ–≤–µ–Ω—å —ç–Ω–µ—Ä–≥–∏–∏ —Å–ª–æ—è –¥–ª—è –¥–µ–ª–µ–Ω–∏—è
    
    # –ß–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π (–≤ —à–∞–≥–∞—Ö —Å–∏–º—É–ª—è—Ü–∏–∏)
    update_interval: int = 100         # –ù–µ —Ä–µ—Å–∞–π–∑–∏–º —Ç–µ–Ω–∑–æ—Ä—ã –∫–∞–∂–¥—ã–π —Ç–∏–∫ (–¥–æ—Ä–æ–≥–æ)

@dataclass
class ChemistryConfig:
    """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–µ–π—Ä–æ–º–æ–¥—É–ª—è—Ü–∏–∏ –∏ –∞–Ω–µ—Å—Ç–µ–∑–∏–∏"""
    lambda_decay: float = 0.01
    
    # GABA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    e_cl: float = -0.070
    e_k: float = -0.090
    tau_gaba_a: float = 0.010
    tau_gaba_b: float = 0.200
    
    # –ê–Ω–µ—Å—Ç–µ–∑–∏—è
    k_propofol: float = 8.0

# ==========================================
# 2. TOPOLOGY GENERATOR (Small-World)
# ==========================================

class TopologyGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–æ–ø–æ–ª–æ–≥–∏–∏ Small-World —Å –º–æ–¥—É–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
    
    @staticmethod
    def create_small_world_mask(n_pre: int, n_post: int, cfg: TopologyConfig, 
                                 layer_type: str = "bottom_up") -> torch.Tensor:
        """
        –°–æ–∑–¥–∞—ë—Ç –º–∞—Å–∫—É —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –¥–ª—è Small-World —Å–µ—Ç–∏.
        
        Args:
            n_pre: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ—Å–∏–Ω–∞–ø—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤
            n_post: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç—Å–∏–Ω–∞–ø—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤
            cfg: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–ø–æ–ª–æ–≥–∏–∏
            layer_type: —Ç–∏–ø —Å–ª–æ—è ("bottom_up" –∏–ª–∏ "top_down")
        
        Returns:
            mask: –±–∏–Ω–∞—Ä–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ [n_post, n_pre]
        """
        mask = torch.zeros(n_post, n_pre)
        k = cfg.bottom_up_locality if layer_type == "bottom_up" else cfg.top_down_locality
        
        # 1. –õ–æ–∫–∞–ª—å–Ω–æ–µ –∫–æ–ª—å—Ü–æ (Regular lattice)
        for i in range(n_post):
            center_j = int((i / n_post) * n_pre)
            start = max(0, center_j - k)
            end = min(n_pre, center_j + k + 1)
            mask[i, start:end] = 1.0
        
        # 2. Rewiring (Watts-Strogatz) - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        if cfg.p_rewire > 0:
            existing = torch.nonzero(mask)
            n_rewire = int(len(existing) * cfg.p_rewire)
            if n_rewire > 0:
                samples = existing[torch.randperm(len(existing))[:n_rewire]]
                # Batch —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Å–≤—è–∑–µ–π
                mask[samples[:, 0], samples[:, 1]] = 0.0
                # Batch —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
                new_js = torch.randint(0, n_pre, (n_rewire,))
                mask[samples[:, 0], new_js] = 1.0
        
        # 3. Long-range shortcuts - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        n_shortcuts = int(n_pre * n_post * cfg.shortcut_density)
        if n_shortcuts > 0:
            shortcut_i = torch.randint(0, n_post, (n_shortcuts,))
            shortcut_j = torch.randint(0, n_pre, (n_shortcuts,))
            mask[shortcut_i, shortcut_j] = 1.0
        
        # 4. –ú–æ–¥—É–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
        if cfg.enable_modules:
            mask = TopologyGenerator._add_modules(mask, n_pre, n_post, cfg)
        
        return mask
    
    @staticmethod
    def _add_modules(mask: torch.Tensor, n_pre: int, n_post: int, cfg: TopologyConfig):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –º–æ–¥—É–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –º–µ–∂–º–æ–¥—É–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π)"""
        mod_size_pre = n_pre // cfg.n_modules
        mod_size_post = n_post // cfg.n_modules
        
        for i in range(cfg.n_modules):
            for j in range(cfg.n_modules):
                if i == j: continue  # –í–Ω—É—Ç—Ä–∏–º–æ–¥—É–ª—å–Ω—ã–µ —Å–≤—è–∑–∏ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º
                
                post_s, post_e = i * mod_size_post, (i + 1) * mod_size_post
                pre_s, pre_e = j * mod_size_pre, (j + 1) * mod_size_pre
                
                # –ü–æ–¥–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –¥–æ–ª—é –º–µ–∂–º–æ–¥—É–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π
                suppress = (torch.rand(post_e - post_s, pre_e - pre_s) > cfg.inter_module_sparsity).float()
                mask[post_s:post_e, pre_s:pre_e] *= suppress
        
        return mask

# ==========================================
# 3. –°–û–°–¢–û–Ø–ù–ò–ï –ù–ï–ô–†–û–•–ò–ú–ò–ò
# ==========================================

class BioChemistry:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ –∏ –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è–º–∏ –Ω–µ–π—Ä–æ–º–µ–¥–∏–∞—Ç–æ—Ä–æ–≤.
    """
    def __init__(self, config: ChemistryConfig):
        self.cfg = config
        
        # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–µ–π—Ä–æ–º–æ–¥—É–ª—è—Ç–æ—Ä—ã
        self.dopamine = torch.tensor(0.5, device=DEVICE)
        self.acetylcholine = torch.tensor(0.5, device=DEVICE)
        self.serotonin = torch.tensor(0.5, device=DEVICE)
        self.norepinephrine = torch.tensor(0.5, device=DEVICE)
        
        # –ê–Ω–µ—Å—Ç–µ–∑–∏—è
        self.propofol_conc = torch.tensor(0.0, device=DEVICE)
    
    def get_gaba_conductance_modifier(self) -> torch.Tensor:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–Ω–æ–∂–∏—Ç–µ–ª—å –ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç–∏ GABA_A –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞–Ω–µ—Å—Ç–µ–∑–∏–∏"""
        return 1.0 + self.cfg.k_propofol * self.propofol_conc

    def update(self, dt: float, stress_level: float, reward_prediction_error: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω–µ–π –Ω–µ–π—Ä–æ–º–µ–¥–∏–∞—Ç–æ—Ä–æ–≤"""
        # DA —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ RPE
        delta_da = reward_prediction_error * 0.2 - (self.dopamine - 0.5) * self.cfg.lambda_decay
        self.dopamine = torch.clamp(self.dopamine + delta_da * dt, 0.0, 1.0)
        
        # NE —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ —Å—Ç—Ä–µ—Å—Å
        if not isinstance(stress_level, torch.Tensor):
            stress_level = torch.tensor(stress_level, device=DEVICE)
            
        delta_ne = stress_level * 0.1 - (self.norepinephrine - 0.5) * self.cfg.lambda_decay
        self.norepinephrine = torch.clamp(self.norepinephrine + delta_ne * dt, 0.0, 1.0)

    def get_state(self) -> dict:
        return {
            'dopamine': self.dopamine,
            'acetylcholine': self.acetylcholine,
            'serotonin': self.serotonin,
            'norepinephrine': self.norepinephrine,
            'propofol_conc': self.propofol_conc
        }

    def load_state(self, state: dict):
        self.dopamine = state.get('dopamine', self.dopamine)
        self.acetylcholine = state.get('acetylcholine', self.acetylcholine)
        self.serotonin = state.get('serotonin', self.serotonin)
        self.norepinephrine = state.get('norepinephrine', self.norepinephrine)
        self.propofol_conc = state.get('propofol_conc', self.propofol_conc)

# ==========================================
# 4. –ù–ï–ô–†–û–ù–ù–´–ô –°–õ–û–ô (–í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–ô PyTorch)
# ==========================================

class NeuralLayer:
    """
    –ü—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–ª–æ–π –Ω–µ–π—Ä–æ–Ω–æ–≤ —Å –±–∏–æ—Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –¥–∏–Ω–∞–º–∏–∫–æ–π.
    """
    def __init__(self, layer_id: str, n_neurons: int, phys_cfg: PhysicsConfig, chem_sys: BioChemistry):
        self.id = layer_id
        self.N = n_neurons
        self.p_cfg = phys_cfg
        self.chem = chem_sys
        
        # --- 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è (CRITICAL: Float64) ---
        self.V = (torch.rand(self.N, device=DEVICE) * 0.5 - 0.2).to(dtype=torch.float64)
        self.phase = (torch.rand(self.N, device=DEVICE) * 2 * math.pi).to(dtype=torch.float64)
        
        # --- –ù–ê–¢–£–†–ê–õ–¨–ù–´–ï –ß–ê–°–¢–û–¢–´ ---
        base_freqs = torch.normal(
            self.p_cfg.omega_base, 
            self.p_cfg.omega_std,
            size=(self.N,),
            device=DEVICE
        )
        self.natural_omegas = torch.clamp(
            base_freqs, 
            min=2 * math.pi * 5.0,
            max=2 * math.pi * 100.0
        ).to(dtype=torch.float64)
        
        # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–∞–π–∫–æ–≤ –ü–ï–†–ï–î –ø–µ—Ä–≤—ã–º —à–∞–≥–æ–º ---
        self.spikes = torch.zeros(self.N, dtype=torch.bool, device=DEVICE)
        self.refractory_timer = torch.zeros(self.N, device=DEVICE)
        self.smoothed_rate = torch.tensor(0.0, device=DEVICE)
        
        # --- –ò–Ω—Ç–µ—Ä–Ω–µ–π—Ä–æ–Ω—ã ---
        self.n_interneurons = int(self.N * 0.2)
        self.V_PV = torch.zeros(self.n_interneurons, device=DEVICE)
        self.V_SST = torch.zeros(self.n_interneurons, device=DEVICE)
        self.V_VIP = torch.zeros(self.n_interneurons, device=DEVICE)
        
        # --- –¢–æ–∫–∏ ---
        self.I_exc = torch.zeros(self.N, device=DEVICE) 
        self.I_inh = torch.zeros(self.N, device=DEVICE) 
        self.I_ext = torch.zeros(self.N, device=DEVICE) 
        
        # --- –†–µ—Ü–µ–ø—Ç–æ—Ä—ã ---
        self.s_gaba_a = torch.zeros(self.N, device=DEVICE) 
        self.s_gaba_b = torch.zeros(self.N, device=DEVICE)

        # === ENERGY STATE ===
        self.ATP = (torch.ones(self.N, device=DEVICE) * self.p_cfg.initial_atp).to(dtype=torch.float64)
        self.is_dead = torch.zeros(self.N, dtype=torch.bool, device=DEVICE)
        
        # === MIRROR INPUT ===
        self.I_mirror = torch.zeros(self.N, device=DEVICE)
        
        # === OPTIMIZATION: –ü—Ä–µ–¥–≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ –±—É—Ñ–µ—Ä—ã –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∞–ª–ª–æ–∫–∞—Ü–∏–π –∫–∞–∂–¥—ã–π —à–∞–≥ ===
        self._noise_buffer = torch.zeros(self.N, device=DEVICE)
        self._I_global_buffer = torch.zeros(self.N, device=DEVICE)
        self._interneuron_noise = torch.zeros(self.n_interneurons, device=DEVICE)
        self._computation_buffer = torch.zeros(self.N, device=DEVICE)

    def compute_kuramoto_order(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –ø–æ—Ä—è–¥–∫–∞ Kuramoto: z = (1/N) * Œ£ exp(i*Œ∏_j)
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (R, Œ®) –≥–¥–µ R - —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è [0,1], Œ® - —Å—Ä–µ–¥–Ω—è—è —Ñ–∞–∑–∞
        """
        complex_phases = torch.exp(1j * self.phase)
        z = torch.mean(complex_phases)
        
        R = torch.abs(z)
        Psi = torch.angle(z)
        
        return R, Psi

    @torch.no_grad()
    def dynamics_step(self, dt: float, global_context: Optional[torch.Tensor] = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–ª–æ—è - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: JIT Kernel + Zero-Copy Buffers"""
        # DEBUG: Check devices
        # print(f"DEBUG: V device: {self.V.device}")
        # print(f"DEBUG: natural_omegas device: {self.natural_omegas.device}")
        # print(f"DEBUG: dopamine device before cast: {self.chem.dopamine.device}")
        
        # Force natural_omegas to correct device if needed
        if self.natural_omegas.device != self.V.device:
             self.natural_omegas = self.natural_omegas.to(self.V.device)


        # Call the fused kernel
        # We ignore 'prev_spikes' (3rd return) and assign 'new_spikes' (4th return) to self.spikes
        (
            self.V, self.phase, _, self.spikes, 
            self.refractory_timer, self.ATP, self.is_dead, 
            self.s_gaba_a, self.s_gaba_b,
            self.V_PV, self.V_SST, self.V_VIP
        ) = fused_layer_step(
            self.V, self.phase, self.spikes, self.refractory_timer, self.ATP, self.is_dead,
            self.s_gaba_a, self.s_gaba_b,
            self.V_PV, self.V_SST, self.V_VIP,
            self.I_exc, self.I_ext, self.I_mirror,
            self.chem.dopamine.to(self.V.device), self.chem.acetylcholine.to(self.V.device), self.chem.propofol_conc.to(self.V.device),
            global_context,
            dt,
            self.p_cfg.tau_mem, self.p_cfg.v_rest, self.p_cfg.v_threshold, self.p_cfg.refractory_period,
            self.p_cfg.c_spike, self.p_cfg.c_recovery, self.p_cfg.c_baseline, self.p_cfg.critical_atp,
            self.chem.cfg.tau_gaba_a, self.chem.cfg.tau_gaba_b, self.chem.get_gaba_conductance_modifier().to(self.V.device),
            self.p_cfg.coupling_strength, self.p_cfg.alpha_sync,
            self.natural_omegas,
            self.p_cfg.initial_atp
        )
        
        # Rate tracking
        instant_rate = self.spikes.float().mean() / dt
        self.smoothed_rate += (dt / 0.1) * (instant_rate - self.smoothed_rate)

        # –û—á–∏—Å—Ç–∫–∞ –≤—Ö–æ–¥–æ–≤
        self.I_exc.zero_()
        self.I_ext.zero_()
        self.I_mirror.zero_()

    def get_activity_rate(self) -> torch.Tensor:
        return self.smoothed_rate
    
    def get_state_vector(self) -> torch.Tensor:
        return self.V
    
    def validate_state(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–ª–æ—è"""
        issues = []
        
        if not torch.all(torch.isfinite(self.V)):
            issues.append("Non-finite values in membrane potential V")
        
        if not torch.all(torch.isfinite(self.phase)):
            issues.append("Non-finite values in phase")
        
        if torch.any(self.ATP < 0):
            issues.append(f"Negative ATP detected: min={self.ATP.min().item():.4f}")
        
        if torch.any(self.ATP > 1.0):
            issues.append(f"ATP exceeds max: max={self.ATP.max().item():.4f}")
        
        if torch.any(self.spikes & self.is_dead):
            issues.append("Dead neurons are spiking!")
        
        if issues:
            print(f"‚ö†Ô∏è Layer {self.id} validation FAILED:")
            for issue in issues:
                print(f"   - {issue}")
            return False
        
        return True

    def get_state(self) -> dict:
        return {
            'V': self.V,
            'phase': self.phase,
            'ATP': self.ATP,
            'natural_omegas': self.natural_omegas,
            'spikes': self.spikes,
            'refractory_timer': self.refractory_timer,
            's_gaba_a': self.s_gaba_a,
            's_gaba_b': self.s_gaba_b,
            'smoothed_rate': self.smoothed_rate,
            'is_dead': self.is_dead
        }

    def load_state(self, state: dict):
        device = self.V.device
        for key, value in state.items():
            if hasattr(self, key):
                target = getattr(self, key)
                if isinstance(target, torch.Tensor) and isinstance(value, torch.Tensor):
                    if target.shape == value.shape:
                        target.copy_(value.to(device))
                elif isinstance(target, torch.Tensor) and not isinstance(value, torch.Tensor):
                     target.fill_(value)

# ==========================================

class DynamicNeuralLayer(NeuralLayer):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ NeuralLayer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–Ω—Ç–æ–≥–µ–Ω–µ–∑–∞ (–∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞).
    """
    def __init__(self, layer_id: str, initial_size: int, phys_cfg: PhysicsConfig, chem_sys: BioChemistry):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω–µ–π—Ä–æ–Ω–æ–≤
        super().__init__(layer_id, initial_size, phys_cfg, chem_sys)
        self.max_capacity = 10000 # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è –∞–ª–ª–æ–∫–∞—Ü–∏–∏ –±—É—Ñ–µ—Ä–æ–≤ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

    def add_neurons(self, count: int):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç 'count' –Ω–æ–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤, —Å–æ—Ö—Ä–∞–Ω—è—è —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö.
        """
        current_N = self.N
        new_N = current_N + count
        
        # 1. –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        # –ú—ã —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏ –∫–æ–ø–∏—Ä—É–µ–º —Ç—É–¥–∞ —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
        # –≠—Ç–æ –æ–ø–µ—Ä–∞—Ü–∏—è GPU-to-GPU, –æ–Ω–∞ –±—ã—Å—Ç—Ä–∞—è, –Ω–æ –Ω–µ –º–≥–Ω–æ–≤–µ–Ω–Ω–∞—è.
        
        # Helper –¥–ª—è —Ä–µ—Å–∞–π–∑–∞ 1D —Ç–µ–Ω–∑–æ—Ä–æ–≤
        def resize_1d(tensor, fill_value=0.0, std=0.0):
            new_tensor = torch.zeros(new_N, device=DEVICE, dtype=tensor.dtype)
            new_tensor[:current_N] = tensor
            if std > 0:
                new_tensor[current_N:] = torch.normal(mean=fill_value, std=std, size=(count,), device=DEVICE)
            else:
                new_tensor[current_N:] = fill_value
            return new_tensor

        # -- –†–µ—Å–∞–π–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö --
        self.V = resize_1d(self.V, fill_value=self.p_cfg.v_rest, std=0.1)
        self.phase = resize_1d(self.phase, fill_value=0.0, std=math.pi) # –ù–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω—ã —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
        
        # –ù–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è –Ω–æ–≤—ã—Ö –Ω–µ–π—Ä–æ–Ω–æ–≤
        new_omegas = torch.normal(
            self.p_cfg.omega_base, 
            self.p_cfg.omega_std, 
            size=(count,), 
            device=DEVICE
        ).to(dtype=torch.float64)
        
        self.natural_omegas = torch.cat([self.natural_omegas, new_omegas])
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –±—É—Ñ–µ—Ä—ã
        self.spikes = torch.cat([self.spikes, torch.zeros(count, dtype=torch.bool, device=DEVICE)])
        self.refractory_timer = resize_1d(self.refractory_timer)
        self.s_gaba_a = resize_1d(self.s_gaba_a)
        self.s_gaba_b = resize_1d(self.s_gaba_b)
        
        # –≠–Ω–µ—Ä–≥–∏—è: –Ω–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω—ã —Ä–æ–∂–¥–∞—é—Ç—Å—è —Å –ø–æ–ª–Ω—ã–º –∑–∞–ø–∞—Å–æ–º ATP
        self.ATP = resize_1d(self.ATP, fill_value=self.p_cfg.initial_atp)
        self.is_dead = torch.cat([self.is_dead, torch.zeros(count, dtype=torch.bool, device=DEVICE)])
        
        # –í—Ö–æ–¥–Ω—ã–µ –±—É—Ñ–µ—Ä—ã (–≤–∞–∂–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å)
        self.I_exc = torch.zeros(new_N, device=DEVICE)
        self.I_ext = torch.zeros(new_N, device=DEVICE)
        self.I_mirror = torch.zeros(new_N, device=DEVICE)
        
        # –ò–Ω—Ç–µ—Ä–Ω–µ–π—Ä–æ–Ω—ã (–º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ, –∏–ª–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏)
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø–æ–∫–∞ –Ω–µ —Ä–∞—Å—Ç–∏–º –ø—É–ª –∏–Ω—Ç–µ—Ä–Ω–µ–π—Ä–æ–Ω–æ–≤, –ª–∏–±–æ –¥–æ–±–∞–≤–ª—è–µ–º –ª–æ–≥–∏–∫—É:
        target_interneurons = int(new_N * 0.2)
        if target_interneurons > self.n_interneurons:
            diff = target_interneurons - self.n_interneurons
            self.V_PV = torch.cat([self.V_PV, torch.zeros(diff, device=DEVICE)])
            self.V_SST = torch.cat([self.V_SST, torch.zeros(diff, device=DEVICE)])
            self.V_VIP = torch.cat([self.V_VIP, torch.zeros(diff, device=DEVICE)])
            self.n_interneurons = target_interneurons

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫
        self.N = new_N
        # print(f"  [Neurogenesis] Layer {self.id} grew to {self.N} neurons.")

# 5. –°–ò–ù–ê–ü–¢–ò–ß–ï–°–ö–ê–Ø –ü–õ–ê–°–¢–ò–ß–ù–û–°–¢–¨ (STDP + Small-World)
# ==========================================

class SynapseMatrix:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –º–∞—Ç—Ä–∏—Ü–µ–π –≤–µ—Å–æ–≤ –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ª–æ—è–º–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤.
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π STDP.
    """
    def __init__(self, n_pre: int, n_post: int, chem_sys: BioChemistry,
                 topo_cfg: Optional[TopologyConfig] = None, layer_type: str = "bottom_up"):
        self.n_pre = n_pre
        self.n_post = n_post
        self.chem = chem_sys
        
        # === –¢–û–ü–û–õ–û–ì–ò–Ø ===
        if topo_cfg and topo_cfg.topology_type == "small_world":
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Å–∫—É –∫–∞–∫ bool (1 –±–∞–π—Ç –≤–º–µ—Å—Ç–æ 8)
            raw_mask = TopologyGenerator.create_small_world_mask(n_pre, n_post, topo_cfg, layer_type)
            self.mask = raw_mask.to(device=DEVICE, dtype=torch.bool)  # –≠–∫–æ–Ω–æ–º–∏—è 7 –±–∞–π—Ç –Ω–∞ —Å–∏–Ω–∞–ø—Å!
            
            self.W = torch.abs(torch.randn(n_post, n_pre, device=DEVICE) * math.sqrt(2.0 / n_pre))
            self.W *= self.mask * topo_cfg.local_boost  # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.mask –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
            
            # --- 2. –ó–∞–º–µ–Ω–∞ –ø–ª–æ—Ç–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü (Dense) –Ω–∞ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ (Sparse CSR) ---
            self.W_dense = self.W * self.mask
            self.W_sparse = self.W_dense.to_sparse_csr()
            self.is_sparse = True
            
            density = self.mask.sum().item() / (n_pre * n_post)
            print(f"  üï∏Ô∏è {layer_type}: density={density:.3f}, boost={topo_cfg.local_boost:.1f} (Sparse CSR enabled)")
        else:
            self.W = torch.abs(torch.randn(n_post, n_pre, device=DEVICE) * math.sqrt(2.0 / n_pre))
            self.mask = None
            self.is_sparse = False
        
        self.theta_M = torch.ones(n_post, device=DEVICE) * 0.5
        self.activity_history = torch.zeros(n_post, device=DEVICE)
        self.trace_pre = torch.zeros(n_pre, device=DEVICE)
        self.trace_post = torch.zeros(n_post, device=DEVICE)
        self.tau_stdp = 0.02
        self.learning_rate_base = 0.001
        
        # --- 4. –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å ---
        self.update_interval = 10
        self.step_counter = 0

    def forward(self, pre_spikes: torch.Tensor) -> torch.Tensor:
        if self.is_sparse:
            # –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ –ø–ª–æ—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å–ø–∞–π–∫–æ–≤
            # torch.mv —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ –¥–ª—è CSR –º–∞—Ç—Ä–∏—Ü
            return torch.mv(self.W_sparse, pre_spikes.float())
        else:
            W_safe = torch.nan_to_num(self.W, nan=0.0, posinf=5.0, neginf=0.0)
            pre_safe = torch.nan_to_num(pre_spikes.float(), nan=0.0, posinf=1.0, neginf=0.0)
            return torch.matmul(W_safe, pre_safe)

    def backward(self, post_signal: torch.Tensor) -> torch.Tensor:
        """Back-projects signal from post-synaptic to pre-synaptic space (Transpose)"""
        # Note: ignoring sparsity optimization for backward pass for now, using dense W
        if self.is_sparse:
             return torch.matmul(self.W_dense.t(), post_signal.float())
        else:
             return torch.matmul(self.W.t(), post_signal.float())

    @torch.no_grad()
    @torch.no_grad()
    def update_plasticity(self, dt: float, 
                          pre_spikes: torch.Tensor, post_spikes: torch.Tensor,
                          pre_phase: torch.Tensor, post_phase: torch.Tensor,
                          sleep_stage: str = 'Wake'):
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: Event-Driven STDP + Lazy Sparse Sync"""
        
        if sleep_stage == 'SWS':
            downscale_factor = 0.0001 * dt
            # Optimization: In-place multiplication
            if self.is_sparse:
                 self.W_dense.mul_(1.0 - downscale_factor)
            else:
                 self.W.mul_(1.0 - downscale_factor)
            return

        # 1. Traces (Global update needed for history)
        # Using in-place operations to save memory
        self.trace_pre.mul_(1.0 - dt / self.tau_stdp).add_(pre_spikes.float())
        self.trace_post.mul_(1.0 - dt / self.tau_stdp).add_(post_spikes.float())
        
        # 2. Event-Driven Check: If no post-synaptic spikes, skip expensive matrix ops
        post_spike_indices = torch.nonzero(post_spikes).squeeze(-1)
        if post_spike_indices.numel() == 0:
            return

        # --- 3. Strided / Throttled Updates ---
        self.step_counter += 1
        if self.step_counter % self.update_interval != 0:
            return

        # 4. Sparse/Active-Only Calculation
        # We only need to update rows corresponding to spiking post-neurons
        
        # Gather phases for spiking neurons only [k]
        active_post_phases = post_phase[post_spike_indices]
        
        # Calclulate phase diff for active rows only: [k, 1] - [1, N_pre] -> [k, N_pre]
        phase_diff = active_post_phases.unsqueeze(1) - pre_phase.unsqueeze(0)
        phase_mod = 1.0 + 0.5 * torch.cos(phase_diff)
        
        # Broadcast trace_pre: [1, N_pre]
        # Result delta: [k, N_pre]
        delta_w_active = self.trace_pre.unsqueeze(0) * phase_mod
        
        # [MODIFIED] Bidirectional Dopamine Modulation
        # DA > 0.4 -> LTP (Positive Learning)
        # DA < 0.4 -> LTD (Negative Learning / Avoidance)
        # Pivot is 0.4. Scale factor 5.0 ensures strong reaction.
        da_modulation = (self.chem.dopamine - 0.4) * 5.0
        
        # Base Learning Rate scaled by modulation (can be negative!)
        effective_learning_signal = self.learning_rate_base * da_modulation
        
        # 5. Apply Updates
        if self.is_sparse:
            # We update W_dense but sync to W_sparse lazily
            
            # Extract effective signal for spiking neurons [k]
            # self.theta_M is [N_post], we need [k]
            # Scaling by theta_M (Homeostatic scaling)
            eff_lr_active = effective_learning_signal / (self.theta_M[post_spike_indices] + 0.1)
            
            # Decay factor (Always positive, always reduces weights)
            decay_factor = self.learning_rate_base * 0.1
            
            current_weights_active = self.W_dense[post_spike_indices, :]
            
            # Total delta for active rows:
            # Update = (Signal * Hebbian) - (Decay * CurrentW)
            # If Signal is negative (LTD), Hebbian term reduces weights.
            row_updates = eff_lr_active.unsqueeze(1) * delta_w_active - decay_factor * current_weights_active
            
            # Scatter add to dense matrix
            self.W_dense[post_spike_indices, :] += row_updates
            
            # Clamp in place
            self.W_dense[post_spike_indices, :] = torch.clamp(self.W_dense[post_spike_indices, :], 0.0, 5.0)
            
            # LAZY SYNC: Re-create sparse tensor only occasionally
            if self.step_counter % 100 == 0:
                self.W_dense.masked_fill_(~self.mask, 0.0)
                self.W_sparse = self.W_dense.to_sparse_csr()
                
        else:
            # Dense Case
            eff_lr_active = effective_learning_signal / (self.theta_M[post_spike_indices] + 0.1)
            decay_factor = self.learning_rate_base * 0.1
            
            current_weights_active = self.W[post_spike_indices, :]
            row_updates = eff_lr_active.unsqueeze(1) * delta_w_active - decay_factor * current_weights_active
            
            self.W[post_spike_indices, :] += row_updates
            self.W[post_spike_indices, :] = torch.clamp(self.W[post_spike_indices, :], 0.0, 5.0)

        # 6. Metaplasticity (Only for active neurons)
        sliding_window_alpha = 0.01
        self.activity_history[post_spike_indices] += sliding_window_alpha * (1.0 - self.activity_history[post_spike_indices])
        
        # Optimized Metaplasticity Update
        target_rate = 0.1
        d_theta = 0.001 * (self.activity_history - target_rate)
        self.theta_M = torch.clamp(self.theta_M + d_theta, 0.1, 10.0)

    def get_state(self) -> dict:
        state = {
            'W_dense': self.W_dense if self.is_sparse else self.W,
            'theta_M': self.theta_M,
            'activity_history': self.activity_history,
            'trace_pre': self.trace_pre,
            'trace_post': self.trace_post
        }
        return state

    def load_state(self, state: dict):
        device = self.theta_M.device
        if 'W_dense' in state:
             W_loaded = state['W_dense'].to(device)
             if self.is_sparse:
                 if self.W_dense.shape == W_loaded.shape:
                      self.W_dense.copy_(W_loaded)
                      self.W_sparse = self.W_dense.to_sparse_csr()
             else:
                 if self.W.shape == W_loaded.shape:
                      self.W.copy_(W_loaded)
        
        if 'theta_M' in state: self.theta_M.copy_(state['theta_M'].to(device))
        if 'activity_history' in state: self.activity_history.copy_(state['activity_history'].to(device))
        if 'trace_pre' in state: self.trace_pre.copy_(state['trace_pre'].to(device))
        if 'trace_post' in state: self.trace_post.copy_(state['trace_post'].to(device))

# ==========================================
# 6. –ò–ï–†–ê–†–•–ò–Ø –ò –ü–†–ï–î–ò–ö–¢–ò–í–ù–û–ï –ö–û–î–ò–†–û–í–ê–ù–ò–ï
# ==========================================

class DynamicSynapseMatrix(SynapseMatrix):
    def __init__(self, n_pre: int, n_post: int, chem_sys: BioChemistry,
                 topo_cfg: Optional[TopologyConfig] = None, layer_type: str = "bottom_up"):
        super().__init__(n_pre, n_post, chem_sys, topo_cfg, layer_type)
        self.topo_cfg = topo_cfg
        self.layer_type = layer_type

    def resize_matrix(self, new_n_pre: int, new_n_post: int):
        """
        –ò–∑–º–µ–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤.
        –°—Ç–∞—Ä—ã–µ –≤–µ—Å–∞ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–∞ —Å–≤–æ–∏—Ö –º–µ—Å—Ç–∞—Ö (–≤–µ—Ä—Ö–Ω–∏–π –ª–µ–≤—ã–π —É–≥–æ–ª).
        –ù–æ–≤—ã–µ –≤–µ—Å–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–æ–ø–æ–ª–æ–≥–∏–∏.
        """
        if new_n_pre == self.n_pre and new_n_post == self.n_post:
            return

        old_pre = self.n_pre
        old_post = self.n_post
        
        # 1. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω—É–ª—è–º–∏ (–∏–ª–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π)
        new_W_dense = torch.zeros(new_n_post, new_n_pre, device=DEVICE)
        
        # –ö–æ–ø–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–π –±–ª–æ–∫
        new_W_dense[:old_post, :old_pre] = self.W_dense if self.is_sparse else self.W
        
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ù–û–í–´–• —Å–≤—è–∑–µ–π (Neurogenesis Integration)
        # –ù–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω—ã –¥–æ–ª–∂–Ω—ã —Å—Ä–∞–∑—É –æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å —Å–≤—è–∑–∏, –∏–Ω–∞—á–µ –æ–Ω–∏ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã.
        
        # –∞) –°–≤—è–∑–∏ –¥–ª—è –Ω–æ–≤—ã—Ö Pre-–Ω–µ–π—Ä–æ–Ω–æ–≤ (–Ω–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã)
        if new_n_pre > old_pre:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–≤—è–∑–∏ –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [old_pre : new_n_pre]
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É Small-World: –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å–≤—è–∑–∏ + —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–ª—å–Ω–∏–µ
            added_cols = new_n_pre - old_pre
            
            # –õ–æ–∫–∞–ª—å–Ω–æ—Å—Ç—å: –ø–æ–¥–∫–ª—é—á–∞–µ–º –∫ —Å–ª—É—á–∞–π–Ω—ã–º –Ω–µ–π—Ä–æ–Ω–∞–º –≤ post-—Å–ª–æ–µ
            # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –Ω–æ–≤—ã—Ö —Å–≤—è–∑–µ–π –¥–µ–ª–∞–µ–º —á—É—Ç—å –Ω–∏–∂–µ, —á—Ç–æ–±—ã "–º—è–≥–∫–æ" –≤–æ–π—Ç–∏ –≤ —Å–µ—Ç—å
            density = 0.1 
            mask_new_cols = (torch.rand(new_n_post, added_cols, device=DEVICE) < density).float()
            weights_new_cols = torch.abs(torch.randn(new_n_post, added_cols, device=DEVICE) * 0.05)
            
            new_W_dense[:, old_pre:] = weights_new_cols * mask_new_cols

        # –±) –°–≤—è–∑–∏ –¥–ª—è –Ω–æ–≤—ã—Ö Post-–Ω–µ–π—Ä–æ–Ω–æ–≤ (–Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏)
        if new_n_post > old_post:
            added_rows = new_n_post - old_post
            density = 0.1
            mask_new_rows = (torch.rand(added_rows, new_n_pre, device=DEVICE) < density).float()
            weights_new_rows = torch.abs(torch.randn(added_rows, new_n_pre, device=DEVICE) * 0.05)
            
            new_W_dense[old_post:, :] = weights_new_rows * mask_new_rows

        # 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ STDP
        self.theta_M = self._resize_vector(self.theta_M, new_n_post, fill=0.5)
        self.activity_history = self._resize_vector(self.activity_history, new_n_post, fill=0.0)
        self.trace_pre = self._resize_vector(self.trace_pre, new_n_pre, fill=0.0)
        self.trace_post = self._resize_vector(self.trace_post, new_n_post, fill=0.0)
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.n_pre = new_n_pre
        self.n_post = new_n_post
        
        if self.is_sparse:
            self.W_dense = new_W_dense
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞—Å–∫—É (–¥–ª—è –ª–µ–Ω–∏–≤–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏)
            new_mask = (new_W_dense > 0).bool() # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞—Å–∫—É –∏–∑ –∑–Ω–∞—á–µ–Ω–∏–π
            self.mask = new_mask
            self.W_sparse = self.W_dense.to_sparse_csr()
        else:
            self.W = new_W_dense
            
    def _resize_vector(self, vec, new_size, fill=0.0):
        if vec.shape[0] == new_size: return vec
        new_vec = torch.ones(new_size, device=DEVICE) * fill
        new_vec[:vec.shape[0]] = vec
        return new_vec


class PredictiveUnit:
    """
    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —É–∑–ª–∞ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ Free Energy Principle.
    
    –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:
    1. Prediction: Œº_l = g(X_{l+1}, a)
    2. Prediction Error: Œµ_l = Œ† ¬∑ (Input - Œº_l)
    3. Variational Free Energy: F ‚âà 0.5 * Œµ^T ¬∑ Œ† ¬∑ Œµ + KL_terms
    """
    def __init__(self, name: str, size: int, phys_cfg: PhysicsConfig, chem_sys: BioChemistry):
        self.layer = NeuralLayer(name, size, phys_cfg, chem_sys)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏—è
        self.mu = torch.zeros(size, device=DEVICE)             # Prediction (Œº)
        self.prediction_error = torch.zeros(size, device=DEVICE) # Error (Œµ)
        self.precision = torch.ones(size, device=DEVICE)       # Precision (Œ†) - –æ–±—Ä–∞—Ç–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        
        # –ë—É—Ñ–µ—Ä—ã –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏
        self.input_buffer = torch.zeros(size, device=DEVICE)
        self.top_down_buffer = torch.zeros(size, device=DEVICE)
        
        # –°–∏–Ω–∞–ø—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
        self.synapse_bottom_up: Optional[SynapseMatrix] = None
        self.synapse_top_down: Optional[SynapseMatrix] = None
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏—è (Lateral connection for Efference Copy)
        # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1D –∏–ª–∏ 2D) –≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–ª–æ—è
        self.action_projection = torch.randn(size, 1, device=DEVICE) * 0.1 

    def update_precision(self):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å (Œ†) –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–π—Ä–æ–º–æ–¥—É–ª—è—Ü–∏–∏.
        Œ† ‚àù 1 + [DA] + [ACh] - [NE] (uncertainty)
        """
        da = self.layer.chem.dopamine
        ach = self.layer.chem.acetylcholine
        ne = self.layer.chem.norepinephrine
        
        # NE —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ—Å—Ç–∏/–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ -> —Å–Ω–∏–∂–∞–µ—Ç precision priors
        # DA/ACh –ø–æ–≤—ã—à–∞—é—Ç —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª/—à—É–º
        base_precision = 1.0 + 1.5 * da + 1.0 * ach - 0.8 * ne
        self.precision = torch.clamp(torch.ones_like(self.precision) * base_precision, 0.1, 10.0)

    def calculate_error_and_free_energy(self, bottom_up_input: torch.Tensor) -> float:
        """
        Œµ_l(t) = Œ†_l^{1/2} ¬∑ [Input(t) - Œº_l(t)]
        F = 0.5 * Œ£ Œµ_l¬≤
        """
        # 1. –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å
        self.update_precision()
        
        # 2. –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ
        raw_error = bottom_up_input - self.mu
        
        # 2.5. –ö–†–ò–¢–ò–ß–ù–û: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å—ã—Ä—É—é –æ—à–∏–±–∫—É –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≤–∑—Ä—ã–≤–∞
        # –ú–µ–º–±—Ä–∞–Ω–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–∏–º (~0-10), –Ω–æ –æ—à–∏–±–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–∞–∑—É–º–Ω–æ–π
        raw_error = torch.clamp(raw_error, -5.0, 5.0)
        
        # 3. –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ (Precision-weighted prediction error)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å –∏–∑ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–º–ø–ª–∏—Ç—É–¥—ã –æ—à–∏–±–∫–∏
        self.prediction_error = raw_error * torch.sqrt(self.precision)
        
        # 4. –°–≤–æ–±–æ–¥–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å—É–º–º—É –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –æ—à–∏–±–æ–∫)
        # F = 0.5 * (Input - Œº)^T * Œ† * (Input - Œº)
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        n_neurons = self.prediction_error.shape[0]
        free_energy = 0.5 * torch.sum(self.prediction_error ** 2) / max(n_neurons, 1)
        
        return free_energy

    def update_generative_dynamics(self, dt: float, top_down_input: torch.Tensor, 
                                   action_vector: torch.Tensor,
                                   global_context: Optional[torch.Tensor] = None,
                                   is_dreaming: bool = False):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–ª–æ—è (Œº) —Å —É—á–µ—Ç–æ–º:
        1. Top-Down Prior (X_{l+1})
        2. Bottom-Up Error (Œµ_l, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è –≤ –¥–∏–Ω–∞–º–∏–∫—É V)
        3. Action (Efference Copy)
        """
        # --- 1. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (Generative Function g) ---
        # Œº_l ‚âà W_td * X_{l+1} + W_action * a(t)
        
        # === –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è ===
        if action_vector.numel() > 0:
            action_dim = action_vector.shape[0]
            # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–∏–ª–∞—Å—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å 1 –Ω–∞ 9), –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É
            if self.action_projection.shape[1] != action_dim:
                self.action_projection = torch.randn(
                    self.layer.N, action_dim,
                    device=self.action_projection.device,
                    dtype=self.action_projection.dtype
                ) * 0.1
        # ===============================================================

        eff_copy = torch.matmul(self.action_projection, action_vector.unsqueeze(1)).squeeze() if action_vector.numel() > 0 else 0.0
        
        prior_drive = top_down_input + eff_copy
        
        # --- 2. –î–∏–Ω–∞–º–∏–∫–∞ —Å–ª–æ—è (Update X/Œº) ---
        # dŒº/dt = -‚àÇF/‚àÇŒº = Œµ * Œ† + ...
        # –í –Ω–∞—à–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–π NeuralLayer —É–∂–µ –∏–º–µ–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É dV/dt.
        # –ú—ã –ø–æ–¥–∞–µ–º –æ—à–∏–±–∫—É –∫–∞–∫ –≤–æ–∑–±—É–∂–¥–∞—é—â–∏–π/–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É—é—â–∏–π —Ç–æ–∫.
        
        if is_dreaming:
            # –í–æ —Å–Ω–µ –æ—Ç–∫–ª—é—á–∞–µ–º —Å–µ–Ω—Å–æ—Ä–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é, –ø–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ prior + —à—É–º
            drive = (prior_drive - self.layer.V) + torch.randn_like(self.layer.V) * 0.5
        else:
            # –ë–æ–¥—Ä—Å—Ç–≤–æ–≤–∞–Ω–∏–µ: –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É Prior –∏ Error
            # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è (–≤—Ö–æ–¥ > –æ–∂–∏–¥–∞–Ω–∏—è), –º—ã –¥–æ–ª–∂–Ω—ã –ø–æ–≤—ã—Å–∏—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            correction = self.prediction_error * torch.sqrt(self.precision)
            drive = correction + (prior_drive - self.layer.V)

        self.layer.I_ext += drive
        
        # –®–∞–≥ —Ñ–∏–∑–∏–∫–∏ –Ω–µ–π—Ä–æ–Ω–æ–≤
        self.layer.dynamics_step(dt, global_context=global_context)
        
        # –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–ª–æ—è —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º (Œº) –¥–ª—è —É—Ä–æ–≤–Ω—è –Ω–∏–∂–µ
        self.mu = self.layer.get_state_vector()

    def get_state(self) -> dict:
        state = {
            'layer': self.layer.get_state(),
            'mu': self.mu,
            'precision': self.precision,
            'action_projection': self.action_projection
        }
        if self.synapse_bottom_up:
            state['synapse_bottom_up'] = self.synapse_bottom_up.get_state()
        if self.synapse_top_down:
            state['synapse_top_down'] = self.synapse_top_down.get_state()
        return state

    def load_state(self, state: dict):
        device = self.mu.device
        if 'layer' in state:
            self.layer.load_state(state['layer'])
        if 'mu' in state: self.mu.copy_(state['mu'].to(device))
        if 'precision' in state: self.precision.copy_(state['precision'].to(device))
        if 'action_projection' in state: 
             proj = state['action_projection'].to(device)
             if self.action_projection.shape == proj.shape:
                 self.action_projection.copy_(proj)
             else:
                 self.action_projection = proj
                 
        if 'synapse_bottom_up' in state and self.synapse_bottom_up:
            self.synapse_bottom_up.load_state(state['synapse_bottom_up'])
        if 'synapse_top_down' in state and self.synapse_top_down:
            self.synapse_top_down.load_state(state['synapse_top_down'])

class HippocampalSystem:
    def __init__(self, capacity: int = 1000, replay_strength: float = 5.0):
        self.capacity = capacity
        self.buffer: List[torch.Tensor] = []
        self.replay_strength = replay_strength
        
    def store(self, state: torch.Tensor, valence: float):
        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º —Ç–æ–ª—å–∫–æ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–∫—Ä–∞—à–µ–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
        if abs(valence) > 0.3:
            self.buffer.append(state.detach().clone())
            if len(self.buffer) > self.capacity:
                self.buffer.pop(0)
    
    def replay_sws(self, target_layer: NeuralLayer, dt: float) -> bool:
        """–í–ø—Ä—ã—Å–∫–∏–≤–∞–µ—Ç –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–µ –≤ –∫–æ—Ä—É –≤–æ –≤—Ä–µ–º—è –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ —Å–Ω–∞"""
        if len(self.buffer) == 0: return False
        
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–∏–ø–ø–ø–ª–∞ (SWR) –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ
        if torch.rand(1).item() < 0.05:
            idx = torch.randint(0, len(self.buffer), (1,)).item()
            memory = self.buffer[idx]
            
            # –ü–æ–¥–≥–æ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            if memory.shape[0] != target_layer.N:
                min_dim = min(memory.shape[0], target_layer.N)
                current_input = torch.zeros(target_layer.N, device=DEVICE)
                current_input[:min_dim] = (memory[:min_dim] - target_layer.V[:min_dim])
            else:
                current_input = (memory - target_layer.V)
            
            # –ò–Ω—ä–µ–∫—Ü–∏—è —Ç–æ–∫–∞
            target_layer.I_ext += current_input * self.replay_strength
            return True
        return False


class BrainHierarchy:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏–µ–π –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö –±–ª–æ–∫–æ–≤.
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–¥–∞—á—É —Å–æ–æ–±—â–µ–Ω–∏–π (Bottom-Up Errors, Top-Down Predictions, Action Context).
    """
    def __init__(self, phys_cfg: PhysicsConfig, chem_sys: BioChemistry, 
                 topo_cfg: Optional[TopologyConfig] = None, input_dim: int = 2000):
        # OPTIMIZED CONFIGURATION for RTX 4070 (~62000 neurons total)
        sizes = [input_dim, 4000, 3000, 2000] 
        names = ["V1_Sensory", "V2_Association", "IT_Object", "PFC_Executive"]
        
        self.levels: List[PredictiveUnit] = []
        
        # Input Projection (Optional, defaults to Identity/None if input matches V1)
        self.input_projection: Optional[SynapseMatrix] = None
        
        for name, size in zip(names, sizes):
            unit = PredictiveUnit(name, size, phys_cfg, chem_sys)
            self.levels.append(unit)
            
        self.connections = []
        for i in range(len(self.levels) - 1):
            lower = self.levels[i]
            higher = self.levels[i+1]
            
            # –°–≤—è–∑–∏ Small-World
            bu_syn = SynapseMatrix(lower.layer.N, higher.layer.N, chem_sys, topo_cfg, "bottom_up")
            lower.synapse_bottom_up = bu_syn 
            
            td_syn = SynapseMatrix(higher.layer.N, lower.layer.N, chem_sys, topo_cfg, "top_down")
            higher.synapse_top_down = td_syn
            
            self.connections.append((bu_syn, td_syn))

    def process_sensory_input(self, sensory_input: torch.Tensor, 
                              action_vector: torch.Tensor,
                              dt: float, 
                              sleep_stage: str, 
                              global_context: Optional[torch.Tensor] = None):
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–∏:
        1. –†–∞—Å—á–µ—Ç –æ—à–∏–±–æ–∫ (Bottom-Up Pass)
        2. –î–∏–Ω–∞–º–∏–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏–π (Top-Down + Lateral Action Pass)
        3. –ü–µ—Ä–µ–¥–∞—á–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –º–µ–∂–¥—É —Å–ª–æ—è–º–∏
        """

        is_dreaming = (sleep_stage == 'REM')
        scaling_factor = 1.0

        # === –§–ê–ó–ê 1: –†–ê–°–ß–ï–¢ –û–®–ò–ë–û–ö –ò –°–í–û–ë–û–î–ù–û–ô –≠–ù–ï–†–ì–ò–ò ===
        
        # Apply Input Projection if exists
        if self.input_projection:
             v1_input = self.input_projection.forward(sensory_input)
        else:
             v1_input = sensory_input

        # –°–Ω–∞—á–∞–ª–∞ –Ω–∏–∂–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –≤—Ö–æ–¥ —Å –ø—Ä–æ–≥–Ω–æ–∑–æ–º –ø—Ä–æ—à–ª–æ–≥–æ —à–∞–≥–∞
        self.levels[0].calculate_error_and_free_energy(v1_input)
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç –ø—Ä–∏—à–µ–¥—à–∏–π —Å–Ω–∏–∑—É —Å–∏–≥–Ω–∞–ª (Error Signal) —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º –ø—Ä–æ–≥–Ω–æ–∑–æ–º
        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –í –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–º PC —Å–ª–æ–π L –ø–æ–ª—É—á–∞–µ—Ç –æ—à–∏–±–∫—É –æ—Ç L-1 –∏ –ø–æ—Å—ã–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –≤ L-1.
        # –ó–¥–µ—Å—å –º—ã —É–ø—Ä–æ—â–∞–µ–º: delayed_bottom_up - —ç—Ç–æ –ø—Ä–æ–µ–∫—Ü–∏—è –æ—à–∏–±–∫–∏ —Å–Ω–∏–∑—É.
        for i in range(1, len(self.levels)):
            unit = self.levels[i]
            unit.calculate_error_and_free_energy(unit.input_buffer)

        # === –§–ê–ó–ê 2: –û–ë–ù–û–í–õ–ï–ù–ò–ï –ì–ï–ù–ï–†–ê–¢–ò–í–ù–û–ô –ú–û–î–ï–õ–ò ===
        # –û–±–Ω–æ–≤–ª—è–µ–º Œº (V) –Ω–∞ –æ—Å–Ω–æ–≤–µ priors, errors –∏ efference copy
        for i in range(len(self.levels)):
            unit = self.levels[i]
            
            # PFC (–≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å) –Ω–µ –∏–º–µ–µ—Ç –≤—Ö–æ–¥–∞ —Å–≤–µ—Ä—Ö—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º 0 –∏–ª–∏ memory
            if i == len(self.levels) - 1:
                prior = torch.zeros(unit.layer.N, device=DEVICE)
            else:
                prior = unit.top_down_buffer

            unit.update_generative_dynamics(
                dt=dt, 
                top_down_input=prior,
                action_vector=action_vector, # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–ø–∏—é –¥–µ–π—Å—Ç–≤–∏—è
                global_context=global_context,
                is_dreaming=is_dreaming
            )

        # === –§–ê–ó–ê 3: –ü–ï–†–ï–î–ê–ß–ê –°–û–û–ë–©–ï–ù–ò–ô (–ú–ï–ñ–°–õ–û–ô–ù–ê–Ø –ö–û–ú–ú–£–ù–ò–ö–ê–¶–ò–Ø) ===
        
        # 3.1 Top-Down Predictions (–°–≤–µ—Ä—Ö—É –≤–Ω–∏–∑)
        for i in range(len(self.levels) - 1, -1, -1):
            if i < len(self.levels) - 1:
                higher_unit = self.levels[i+1]
                td_syn = higher_unit.synapse_top_down
                
                # –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ—Å—ã–ª–∞–µ—Ç—Å—è –≤–Ω–∏–∑
                signal = td_syn.forward(higher_unit.mu.float())
                self.levels[i].top_down_buffer = signal * scaling_factor

        # 3.2 Bottom-Up Errors (–°–Ω–∏–∑—É –≤–≤–µ—Ä—Ö)
        # –ú—ã –ø–µ—Ä–µ–¥–∞–µ–º –æ—à–∏–±–∫—É (prediction error), –∞ –Ω–µ —Å–∞–º–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ!
        for i in range(len(self.levels) - 1):
            lower_unit = self.levels[i]
            bu_syn = self.connections[i][0] 
            
            # –û—à–∏–±–∫–∞ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç—Å—è –≤–≤–µ—Ä—Ö
            signal = bu_syn.forward(lower_unit.prediction_error)
            self.levels[i+1].input_buffer = signal * scaling_factor

        # === –§–ê–ó–ê 4: –ü–õ–ê–°–¢–ò–ß–ù–û–°–¢–¨ ===
        if sleep_stage != 'SWS': # –í–æ –≤—Ä–µ–º—è SWS –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è –∏–ª–∏ –º–µ–Ω—è–µ—Ç—Å—è (Hebb renormalization)
            for i in range(len(self.connections)):
                bu, td = self.connections[i]
                lower = self.levels[i].layer
                higher = self.levels[i+1].layer
                
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—é –æ—à–∏–±–∫–∏
                bu.update_plasticity(dt, lower.spikes, higher.spikes, lower.phase, higher.phase, sleep_stage)
                td.update_plasticity(dt, higher.spikes, lower.spikes, higher.phase, lower.phase, sleep_stage)
            
    def get_global_free_energy(self) -> float:
        total = 0.0
        total_neurons = 0
        for l in self.levels:
            # –°—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
            n_neurons = l.prediction_error.shape[0]
            total += torch.sum(l.prediction_error ** 2).item()
            total_neurons += n_neurons
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤
        return total / max(total_neurons, 1)

    def get_sensory_prediction_error(self) -> torch.Tensor:
        """Returns the prediction error projected back to sensory space"""
        v1_error = self.levels[0].prediction_error
        if self.input_projection:
            return self.input_projection.backward(v1_error)
        else:
            return v1_error

    def get_state(self) -> dict:
        return {
            'levels': [level.get_state() for level in self.levels]
        }
        
    def load_state(self, state: dict):
        if 'levels' in state:
            for i, level_state in enumerate(state['levels']):
                if i < len(self.levels):
                    self.levels[i].load_state(level_state)

class NeurogenesisManager:
    def __init__(self, hierarchy: BrainHierarchy, config: NeurogenesisConfig):
        self.hierarchy = hierarchy
        self.cfg = config
        self.step_counter = 0
        
        # –°—Ç–∞—Ç—É—Å —Ä–∞–∑–≤–∏—Ç–∏—è
        self.phase = "MORPHOGENESIS" # or "MATURATION"
        
        print(f"üß¨ Neurogenesis System Online. Target Base: {self.cfg.base_target_neurons}")

    def update(self, free_energy: float):
        self.step_counter += 1
        if self.step_counter % self.cfg.update_interval != 0:
            return

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —É—Ä–æ–≤–Ω—è–º –∏–µ—Ä–∞—Ä—Ö–∏–∏
        for i, unit in enumerate(self.hierarchy.levels):
            layer = unit.layer # –≠—Ç–æ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å DynamicNeuralLayer
            
            # --- –õ–æ–≥–∏–∫–∞ —Ä–µ—à–µ–Ω–∏–π ---
            growth_needed = 0
            
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ OOM
            if layer.N >= self.cfg.max_neurons_per_layer:
                continue

            # 2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–∑—ã –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
            if layer.N < self.cfg.base_target_neurons:
                # –§–ê–ó–ê 1: –ë—ã—Å—Ç—Ä—ã–π —Ä–æ—Å—Ç
                self.phase = "MORPHOGENESIS"
                growth_needed = self.cfg.growth_rate_fast
            else:
                # –§–ê–ó–ê 2: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–æ—Å—Ç
                self.phase = "MATURATION"
                
                # –†–∞—Å—Ç–µ–º, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—à–∏–±–∫–∞ –≤—ã—Å–æ–∫–∞—è (–Ω—É–∂–Ω—ã –Ω–æ–≤—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –º–∏—Ä–∞)
                # –ò –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–Ω–µ—Ä–≥–∏–∏ (ATP)
                avg_atp = layer.ATP.mean().item()
                if free_energy > self.cfg.error_threshold_trigger and avg_atp > self.cfg.atp_cost_per_neuron:
                    growth_needed = self.cfg.growth_rate_slow
                    # print(f"  [Growth] Layer {layer.id} triggered adaptation (+{growth_needed}) due to FE={free_energy:.2f}")

            # 3. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–æ—Å—Ç–∞
            if growth_needed > 0:
                print(f"  [Neurogenesis] Layer '{layer.id}' growing: {layer.N} -> {layer.N + growth_needed} (Reason: {self.phase}, FE={free_energy:.2f})")
                self._execute_growth(unit, i, growth_needed)

    def _execute_growth(self, unit, level_idx, count):
        # 1. –†–∞—Å—Ç–∏–º —Å–∞–º —Å–ª–æ–π (–Ω–µ–π—Ä–æ–Ω—ã)
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ç–∏–ø—É DynamicNeuralLayer –¥–ª—è IDE, –ø–æ —Ñ–∞–∫—Ç—É –æ–Ω —Ç–∞–º –∏ –µ—Å—Ç—å
        if isinstance(unit.layer, DynamicNeuralLayer):
            unit.layer.add_neurons(count)
        
        # 2. –†–µ—Å–∞–π–∑–∏–º –±—É—Ñ–µ—Ä—ã –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        new_size = unit.layer.N
        
        # –†–µ—Å–∞–π–∑ mu, error, precision
        def resize_vec(v):
            new_v = torch.zeros(new_size, device=DEVICE)
            new_v[:v.shape[0]] = v
            return new_v
            
        unit.mu = resize_vec(unit.mu)
        unit.prediction_error = resize_vec(unit.prediction_error)
        unit.precision = resize_vec(unit.precision)
        unit.precision[unit.precision == 0] = 1.0 # –ù–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –±–∞–∑–æ–≤—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
        
        unit.input_buffer = resize_vec(unit.input_buffer)
        unit.top_down_buffer = resize_vec(unit.top_down_buffer)
        
        # –†–µ—Å–∞–π–∑ action projection
        if unit.action_projection.numel() > 0:
            act_dim = unit.action_projection.shape[1]
            new_proj = torch.randn(new_size, act_dim, device=DEVICE) * 0.1
            new_proj[:unit.action_projection.shape[0], :] = unit.action_projection
            unit.action_projection = new_proj

        # 3. –†–µ—Å–∞–π–∑–∏–º –°–∏–Ω–∞–ø—Å—ã (Corrected Logic)
        
        # A) –ò—Å—Ö–æ–¥—è—â–∏–µ —Å–≤—è–∑–∏ (Unit -> Outputs)
        # Unit —è–≤–ª—è–µ—Ç—Å—è PRE-—Å–∏–Ω–∞–ø—Ç–∏—á–µ—Å–∫–∏–º –¥–ª—è —Å–≤–æ–∏—Ö .synapse_bottom_up –∏ .synapse_top_down
        
        # Unit -> Higher (Bottom-Up Output)
        if unit.synapse_bottom_up and isinstance(unit.synapse_bottom_up, DynamicSynapseMatrix):
            # Pre=Unit(Grew), Post=Higher(Fixed)
            # print(f"    -> Resizing Output (Bottom-Up) to {unit.synapse_bottom_up.n_post}x{new_size}")
            unit.synapse_bottom_up.resize_matrix(new_size, unit.synapse_bottom_up.n_post)

        # Unit -> Lower (Top-Down Output)
        if unit.synapse_top_down and isinstance(unit.synapse_top_down, DynamicSynapseMatrix):
            # Pre=Unit(Grew), Post=Lower(Fixed)
            unit.synapse_top_down.resize_matrix(new_size, unit.synapse_top_down.n_post)

        # B) –í—Ö–æ–¥—è—â–∏–µ —Å–≤—è–∑–∏ (Inputs -> Unit)
        # Unit —è–≤–ª—è–µ—Ç—Å—è POST-—Å–∏–Ω–∞–ø—Ç–∏—á–µ—Å–∫–∏–º –¥–ª—è —Å–≤—è–∑–µ–π —Å–æ—Å–µ–¥–µ–π
        
        # Lower -> Unit (Bottom-Up Input)
        if level_idx > 0:
            lower_unit = self.hierarchy.levels[level_idx - 1]
            if lower_unit.synapse_bottom_up and isinstance(lower_unit.synapse_bottom_up, DynamicSynapseMatrix):
                 # Pre=Lower(Fixed), Post=Unit(Grew)
                 lower_unit.synapse_bottom_up.resize_matrix(lower_unit.synapse_bottom_up.n_pre, new_size)
        
        # Higher -> Unit (Top-Down Input)
        if level_idx < len(self.hierarchy.levels) - 1:
            higher_unit = self.hierarchy.levels[level_idx + 1]
            if higher_unit.synapse_top_down and isinstance(higher_unit.synapse_top_down, DynamicSynapseMatrix):
                 # Pre=Higher(Fixed), Post=Unit(Grew)
                 higher_unit.synapse_top_down.resize_matrix(higher_unit.synapse_top_down.n_pre, new_size)

        # 4. Resize Input Projection if V1 grew (Special Case Input)
        if level_idx == 0 and self.hierarchy.input_projection:
             if isinstance(self.hierarchy.input_projection, DynamicSynapseMatrix):
                   # Pre=Sensory(Fixed), Post=V1(Grew)
                   self.hierarchy.input_projection.resize_matrix(
                        self.hierarchy.input_projection.n_pre, 
                        new_size
                   )

class DynamicBrainHierarchy(BrainHierarchy):
    def __init__(self, phys_cfg: PhysicsConfig, chem_sys: BioChemistry, 
                 topo_cfg: Optional[TopologyConfig], neuro_cfg: NeurogenesisConfig):
        
        # –í–ê–ñ–ù–û: –°—Ç–∞—Ä—Ç—É–µ–º —Å –º–∞–ª–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (initial_neurons), –∞ –Ω–µ —Å –ø–æ–ª–Ω–æ–≥–æ
        init_size = neuro_cfg.initial_neurons
        
        # –ò–º–µ–Ω–∞ —Å–ª–æ–µ–≤ —Ç–µ –∂–µ
        names = ["V1_Sensory", "V2_Association", "IT_Object", "PFC_Executive"]
        
        self.levels: List[PredictiveUnit] = []
        
        # Initialize Input Projection (Sensory -> V1)
        # Pre=Sensory(Fixed=2000), Post=V1(Growing=init_size)
        input_dim = 2000 
        self.input_projection = DynamicSynapseMatrix(
            n_pre=input_dim, 
            n_post=init_size, 
            chem_sys=chem_sys,
            topo_cfg=topo_cfg,
            layer_type="bottom_up"
        )
        
        for name in names:
            # –°–æ–∑–¥–∞–µ–º Unit, –Ω–æ –≤–Ω—É—Ç—Ä–∏ –ø–æ–¥–º–µ–Ω—è–µ–º NeuralLayer –Ω–∞ DynamicNeuralLayer
            unit = PredictiveUnit(name, init_size, phys_cfg, chem_sys)
            # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ—è
            unit.layer = DynamicNeuralLayer(name, init_size, phys_cfg, chem_sys)
            
            # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ action_projection –ø–æ–¥ –Ω–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
            unit.action_projection = torch.randn(init_size, 1, device=DEVICE) * 0.1
            
            # –†–µ—Å–∞–π–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –±—É—Ñ–µ—Ä–æ–≤ unit'–∞ –ø–æ–¥ init_size
            unit.mu = torch.zeros(init_size, device=DEVICE)
            unit.prediction_error = torch.zeros(init_size, device=DEVICE)
            unit.precision = torch.ones(init_size, device=DEVICE)
            unit.input_buffer = torch.zeros(init_size, device=DEVICE)
            unit.top_down_buffer = torch.zeros(init_size, device=DEVICE)
            
            self.levels.append(unit)
            
        self.connections = []
        # –°–æ–∑–¥–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —Å–∏–Ω–∞–ø—Å—ã
        for i in range(len(self.levels) - 1):
            lower = self.levels[i]
            higher = self.levels[i+1]
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DynamicSynapseMatrix
            bu_syn = DynamicSynapseMatrix(lower.layer.N, higher.layer.N, chem_sys, topo_cfg, "bottom_up")
            lower.synapse_bottom_up = bu_syn 
            
            td_syn = DynamicSynapseMatrix(higher.layer.N, lower.layer.N, chem_sys, topo_cfg, "top_down")
            higher.synapse_top_down = td_syn
            
            self.connections.append((bu_syn, td_syn))

    def load_state(self, state: dict):
        # 1. Resize layers to match saved state
        if 'levels' in state:
            for i, level_state in enumerate(state['levels']):
                if i >= len(self.levels): break
                
                # Deduce saved size
                target_size = -1
                if 'mu' in level_state:
                     target_size = level_state['mu'].shape[0]
                elif 'layer' in level_state and 'V' in level_state['layer']: 
                     target_size = level_state['layer']['V'].shape[0]
                
                current_size = self.levels[i].layer.N
                
                if target_size > current_size:
                    print(f"üì• Loading Brain: Resizing Layer {i} ({current_size} -> {target_size})")
                    diff = target_size - current_size
                    self._resize_level(level_idx=i, count=diff)
        
        # 2. Standard Load
        super().load_state(state)

    def _resize_level(self, level_idx, count):
        unit = self.levels[level_idx]
        
        # 1. Grow Neurons
        if hasattr(unit.layer, 'add_neurons'):
             unit.layer.add_neurons(count)
             
        new_size = unit.layer.N
        
        # 2. Resize Predictive Buffers
        def resize_vec(v):
            if v.shape[0] == new_size: return v
            new_v = torch.zeros(new_size, device=DEVICE)
            min_len = min(v.shape[0], new_size)
            new_v[:min_len] = v[:min_len]
            return new_v
            
        unit.mu = resize_vec(unit.mu)
        unit.prediction_error = resize_vec(unit.prediction_error)
        unit.precision = resize_vec(unit.precision)
        unit.precision[unit.precision == 0] = 1.0 
        
        unit.input_buffer = resize_vec(unit.input_buffer)
        unit.top_down_buffer = resize_vec(unit.top_down_buffer)
        
        # Action Projection
        if unit.action_projection.numel() > 0:
            act_dim = unit.action_projection.shape[1]
            new_proj = torch.randn(new_size, act_dim, device=DEVICE) * 0.1
            min_len = min(unit.action_projection.shape[0], new_size)
            new_proj[:min_len, :] = unit.action_projection[:min_len, :]
            unit.action_projection = new_proj

        # 3. Resize Synapses
        # Output Bottom-Up
        if unit.synapse_bottom_up and hasattr(unit.synapse_bottom_up, 'resize_matrix'):
            unit.synapse_bottom_up.resize_matrix(new_size, unit.synapse_bottom_up.n_post)
            
        # Output Top-Down
        if unit.synapse_top_down and hasattr(unit.synapse_top_down, 'resize_matrix'):
            unit.synapse_top_down.resize_matrix(new_size, unit.synapse_top_down.n_post)
            
        # Input Bottom-Up (from Lower)
        if level_idx > 0:
            lower = self.levels[level_idx - 1]
            if lower.synapse_bottom_up and hasattr(lower.synapse_bottom_up, 'resize_matrix'):
                 lower.synapse_bottom_up.resize_matrix(lower.synapse_bottom_up.n_pre, new_size)
                 
        # Input Top-Down (from Higher)
        if level_idx < len(self.levels) - 1:
            higher = self.levels[level_idx + 1]
            if higher.synapse_top_down and hasattr(higher.synapse_top_down, 'resize_matrix'):
                 higher.synapse_top_down.resize_matrix(higher.synapse_top_down.n_pre, new_size)
                 
        # Input Projection (V1)
        if level_idx == 0 and self.input_projection:
             if hasattr(self.input_projection, 'resize_matrix'):
                  self.input_projection.resize_matrix(self.input_projection.n_pre, new_size)

# ==========================================
# 7. –ì–õ–û–ë–ê–õ–¨–ù–û–ï –†–ê–ë–û–ß–ï–ï –ü–†–û–°–¢–†–ê–ù–°–¢–í–û (GWT)
# ==========================================

class GlobalWorkspace:
    def __init__(self, hierarchy: BrainHierarchy):
        self.hierarchy = hierarchy
        self.theta_activity = 5.0
        self.theta_sync = 0.8      
        self.capacity = 4
        
        self.active_coalitions: List[str] = [] 
        self.broadcast_signal = torch.zeros(100, device=DEVICE)
        self.phi_current = 0.0

        # ID –æ–±–ª–∞—Å—Ç–µ–π + Social + Agency
        self.area_ids = [u.layer.id for u in self.hierarchy.levels] + ['Social_Self', 'Sense_of_Agency']
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –∫—ç—à –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è O(1) –ø–æ–∏—Å–∫–∞ –≤–º–µ—Å—Ç–æ O(n) list.index()
        self.area_id_to_idx = {name: i for i, name in enumerate(self.area_ids)}
        n = len(self.area_ids)
        # –°–ª—É—á–∞–π–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–≤ –∏–¥–µ–∞–ª–µ –¥–æ–ª–∂–Ω–∞ –æ–±—É—á–∞—Ç—å—Å—è)
        self.semantic_matrix = torch.rand(n, n, device=DEVICE)
        self.semantic_matrix = (self.semantic_matrix + self.semantic_matrix.T) / 2.0

    def step(self, dt: float, social_module: Optional['SocialCognition'] = None):
        candidates = []
        
        # 1. –°–±–æ—Ä –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        for unit in self.hierarchy.levels:
            activity = unit.layer.get_activity_rate().item()
            R, _ = unit.layer.compute_kuramoto_order()
            R_val = R.item()
            
            # === –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (Weber-Fechner) ===
            norm_activity = math.log1p(activity / 10.0) * 5.0

            # –£—Å–ª–æ–≤–∏–µ –≤—Ö–æ–¥–∞: –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—à–µ —Ñ–æ–Ω–∞ –∏ –µ—Å—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
            if activity > self.theta_activity and R_val > self.theta_sync:
                candidates.append({
                    'id': unit.layer.id,
                    'score': norm_activity * R_val,
                    'data': unit.layer.get_state_vector()
                })
        
        # 2. –°–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if social_module is not None:
            social_pain = social_module.get_social_pain_signal()
            social_urgency = social_pain * 20.0 

            # –ü–æ–ª—É—á–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ø—Ä–æ–ø–æ—Ñ–æ–ª–∞ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
            propofol = self.hierarchy.levels[0].layer.chem.propofol_conc
            
            # –í—ã—á–∏—Å–ª—è–µ–º "—Ñ–∞–∫—Ç–æ—Ä –±–æ–¥—Ä—Å—Ç–≤–æ–≤–∞–Ω–∏—è" (1.0 -> 0.0)
            awake_factor = torch.clamp(1.0 - (propofol / 3.0), 0.0, 1.0)
            
            # –ì–ª—É—à–∏–º —Å–æ—Ü–∏–∞–ª—å–Ω—É—é —Å—Ä–æ—á–Ω–æ—Å—Ç—å –∞–Ω–µ—Å—Ç–µ–∑–∏–µ–π
            social_urgency *= awake_factor
            
            if social_urgency > 0.5:
                social_vector = social_module.m2_self_in_other
                pad_size = 100 - social_vector.shape[0]
                if pad_size > 0:
                    padded_social = torch.cat([social_vector, torch.zeros(pad_size, device=social_vector.device)])
                else:
                    padded_social = social_vector[:100]
                
                candidates.append({
                    'id': 'Social_Self_Model',
                    'score': social_urgency,
                    'data': padded_social
                })

        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º dict.get() –≤–º–µ—Å—Ç–æ list.index()
        if self.active_coalitions:
            for cand in candidates:
                binding_boost = 0.0
                cand_idx = self.area_id_to_idx.get(cand['id'], -1)
                
                if cand_idx >= 0:
                    for active_id in self.active_coalitions:
                        act_idx = self.area_id_to_idx.get(active_id, -1)
                        if act_idx >= 0:
                            binding_boost += self.semantic_matrix[cand_idx, act_idx].item()
                
                cand['score'] += binding_boost * 2.0

        # 3. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è
        candidates.sort(key=lambda x: x['score'], reverse=True)
        winners = candidates[:self.capacity]
        self.active_coalitions = [w['id'] for w in winners]
        
        # 4. –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–µ—â–∞–Ω–∏–µ –∏ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        target_phi = 0.0
        
        if winners:
            winner_tensors = [w['data'][:100] for w in winners]
            stacked = torch.stack(winner_tensors)
            combined_signal = torch.mean(stacked, dim=0)
            
            self.broadcast_signal = self.broadcast_signal * 0.9 + combined_signal * 0.1
            target_phi = sum(w['score'] for w in winners)
        else:
            self.broadcast_signal *= 0.9 
            target_phi = 0.0
            
        # === –ò–ù–ï–†–¶–ò–Ø –°–û–ó–ù–ê–ù–ò–Ø (—ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ) ===
        tau_ignition = 0.250

        alpha = 1.0 - torch.exp(torch.tensor(-dt / tau_ignition))
        alpha = float(alpha.item())

        self.phi_current += alpha * (target_phi - self.phi_current)

    def get_context_feedback(self) -> torch.Tensor:
        return self.broadcast_signal

    def get_state(self) -> dict:
        return {
            'semantic_matrix': self.semantic_matrix
        }
    
    def load_state(self, state: dict):
        if 'semantic_matrix' in state:
            self.semantic_matrix.copy_(state['semantic_matrix'].to(self.semantic_matrix.device))

# ==========================================
# 8. –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê
# ==========================================

class AffectiveSystem:
    def __init__(self, chem: BioChemistry):
        self.chem = chem
        
        self.valence = torch.tensor(0.0, device=DEVICE) 
        self.arousal = torch.tensor(0.0, device=DEVICE) 
        
        self.amygdala_activity = torch.tensor(0.0, device=DEVICE) 
        self.insula_activity = torch.tensor(0.0, device=DEVICE)    
        self.vmPFC_activity = torch.tensor(0.0, device=DEVICE)     
        
        self.tau_valence = 5.0  
        self.tau_arousal = 2.0
        
    def update(self, dt: float, 
               total_free_energy: float, 
               body_pain: float, 
               reward_signal: float,
               simulation_time: float = 0.0):
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –∞–Ω–æ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å ~0.01-1.0)
        if total_free_energy > 10.0:
            effective_free_energy = 0.0
        else:
            effective_free_energy = total_free_energy

        # 1. Amygdala
        target_amygdala = 0.8 * body_pain + 0.5 * math.tanh(effective_free_energy)
        self.vmPFC_activity = 0.5 * (1.0 - self.arousal) 
        effective_threat = target_amygdala * (1.0 - 0.5 * self.vmPFC_activity)
        
        self.amygdala_activity += dt * (effective_threat - self.amygdala_activity)

        # 2. Arousal
        drive_arousal = (
            2.0 * math.tanh(effective_free_energy) +  
            1.0 * self.amygdala_activity +          
            0.5 * self.chem.norepinephrine          
        )
        dA = (1.0 / self.tau_arousal) * (-self.arousal + drive_arousal)
        self.arousal = torch.clamp(self.arousal + dA * dt, 0.0, 1.0)

        # 3. Valence
        expected_reward = 0.0 
        rpe = reward_signal - expected_reward
        
        drive_valence = (
            1.0 * rpe                       
            - 1.0 * body_pain               
            - 0.5 * effective_free_energy   
        )
        
        dV = (1.0 / self.tau_valence) * (-self.valence + drive_valence)
        self.valence = torch.clamp(self.valence + dV * dt, -1.0, 1.0)
        
        # 4. Insula
        self.insula_activity = 0.7 * body_pain + 0.3 * torch.abs(self.valence)

    def get_modulation_factors(self) -> dict:
        return {
            'learning_rate_mod': 1.0 + 0.5 * self.arousal.item(),
            'attention_precision': 1.0 + 1.0 * self.amygdala_activity.item(),
            'gamma_freq_shift': 5.0 * self.arousal.item() 
        }

    def get_state(self) -> dict:
        return {
            'valence': self.valence,
            'arousal': self.arousal,
            'amygdala': self.amygdala_activity,
            'insula': self.insula_activity
        }
    
    def load_state(self, state: dict):
        self.valence = state.get('valence', self.valence)
        self.arousal = state.get('arousal', self.arousal)
        self.amygdala_activity = state.get('amygdala', self.amygdala_activity)
        self.insula_activity = state.get('insula', self.insula_activity)

# ==========================================
# 9. SOCIAL COGNITION
# ==========================================

@dataclass
class MentalModel:
    intentions: torch.Tensor
    beliefs: torch.Tensor
    emotional_state: torch.Tensor 

class SocialCognition:
    """
    Theory of Mind: –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å M0, M1, M2.
    –†–∞–∑–¥–µ–ª 19 –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏.
    """
    def __init__(self, vector_dim: int = 10):
        self.dim = vector_dim
        
        # M0: –ú–æ–¥–µ–ª—å —Å–µ–±—è
        self.m0_intentions = torch.zeros(self.dim, device=DEVICE)
        self.m0_beliefs = torch.zeros(self.dim, device=DEVICE)
        self.m0_emotions = torch.zeros(2, device=DEVICE)
        
        # M1: –ú–æ–¥–µ–ª—å –¥—Ä—É–≥–æ–≥–æ
        self.m1_other_intentions = torch.zeros(self.dim, device=DEVICE)
        self.m1_other_beliefs = torch.zeros(self.dim, device=DEVICE)
        self.m1_other_emotions = torch.zeros(2, device=DEVICE)
        
        # M2: –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å ("—á—Ç–æ –æ–Ω –¥—É–º–∞–µ—Ç –æ–±–æ –º–Ω–µ")
        self.m2_self_in_other = torch.zeros(self.dim, device=DEVICE)
        
        self.inference_rate = 0.1

    def update_self(self, my_actions: torch.Tensor, my_valence: float, my_arousal: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ M0 –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ —ç–º–æ—Ü–∏–π"""
        if my_actions.shape[0] > self.dim:
            my_actions = my_actions[:self.dim]
        elif my_actions.shape[0] < self.dim:
            pad = torch.zeros(self.dim - my_actions.shape[0], device=my_actions.device)
            my_actions = torch.cat([my_actions, pad])
        
        self.m0_intentions += 0.1 * (my_actions - self.m0_intentions)
        self.m0_emotions = torch.tensor([my_valence, my_arousal], device=DEVICE)

    def observe_other(self, observed_behavior: torch.Tensor, dt: float):
        """
        Inverse inference: –Ω–∞–±–ª—é–¥–∞—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ –¥—Ä—É–≥–æ–≥–æ, –¥–µ–ª–∞–µ–º –≤—ã–≤–æ–¥ –æ –µ–≥–æ –Ω–∞–º–µ—Ä–µ–Ω–∏—è—Ö.
        –†–∞–∑–¥–µ–ª 19b –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏.
        """
        if observed_behavior.shape[0] > self.dim:
            observed_behavior = observed_behavior[:self.dim]
        elif observed_behavior.shape[0] < self.dim:
            pad = torch.zeros(self.dim - observed_behavior.shape[0], device=observed_behavior.device)
            observed_behavior = torch.cat([observed_behavior, pad])
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        predicted_behavior = self.m1_other_intentions * 0.8 + self.m1_other_beliefs * 0.2
        prediction_error = observed_behavior - predicted_behavior
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥—Ä—É–≥–æ–≥–æ (Inverse RL)
        self.m1_other_intentions += self.inference_rate * prediction_error * dt
        
        # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –≤—ã–≤–æ–¥ —ç–º–æ—Ü–∏–π –ø–æ —ç–Ω–µ—Ä–≥–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è
        behavior_energy = torch.norm(observed_behavior)
        inferred_arousal = torch.tanh(behavior_energy)
        self.m1_other_emotions[1] = inferred_arousal

    def recursive_update(self, dt: float):
        """
        M2: "–ß—Ç–æ –æ–Ω –¥—É–º–∞–µ—Ç –æ–±–æ –º–Ω–µ?"
        –†–∞–∑–¥–µ–ª 19e –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏.
        """
        # –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É –º–æ–∏–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –æ —Å–µ–±–µ –∏ –º–µ—Ç–∞-–º–æ–¥–µ–ª—å—é
        discrepancy = self.m0_intentions - self.m2_self_in_other
        self.m2_self_in_other += 0.05 * discrepancy * dt

    def get_social_pain_signal(self) -> float:
        """–°–æ—Ü–∏–∞–ª—å–Ω–∞—è –±–æ–ª—å –æ—Ç –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ä–∂–µ–Ω–∏—è"""
        ideal_self = torch.ones(self.dim, device=DEVICE)
        perceived_rejection = torch.norm(ideal_self - self.m2_self_in_other)
        return perceived_rejection.item() * 0.1

    def get_state(self) -> dict:
        return {
            'm0_intentions': self.m0_intentions,
            'm0_beliefs': self.m0_beliefs,
            'm2_self_in_other': self.m2_self_in_other
        }
    
    def load_state(self, state: dict):
        device = self.m0_intentions.device
        if 'm0_intentions' in state: self.m0_intentions.copy_(state['m0_intentions'].to(device))
        if 'm0_beliefs' in state: self.m0_beliefs.copy_(state['m0_beliefs'].to(device))
        if 'm2_self_in_other' in state: self.m2_self_in_other.copy_(state['m2_self_in_other'].to(device))

# ==========================================
# 10. PCI CALCULATOR
# ==========================================

class PCICalculator:
    """
    –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä PCI.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∂–∞—Ç–∏–µ zlib –≤–º–µ—Å—Ç–æ –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ LZ76 –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤.
    """
    @staticmethod
    def compute_pci(spike_matrix: torch.Tensor, window_size: int = None) -> float:
        if window_size is None:
            window_size = spike_matrix.shape[0]
            
        window = spike_matrix[:window_size, :]
        binary_vec_np = window.flatten().byte().cpu().numpy()
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "–¢–∏—à–∏–Ω—É" (Silence check)
        # –ï—Å–ª–∏ —Å–ø–∞–π–∫–æ–≤ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ (< 0.1% –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏), —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å—á–∏—Ç–∞—Ç—å –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ
        spike_density = binary_vec_np.mean()
        if spike_density < 0.001: 
            return 0.0
            
        data_bytes = binary_vec_np.tobytes()
        L = len(binary_vec_np)
        
        # 2. –°–∂–∞—Ç–∏–µ
        compressed = zlib.compress(data_bytes)
        # –í—ã—á–∏—Ç–∞–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–π –æ–≤–µ—Ä—Ö–µ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–∞ zlib (–æ–±—ã—á–Ω–æ –æ–∫–æ–ª–æ 10-12 –±–∞–π—Ç –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö), 
        # —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —à—É–º –Ω–∞ –Ω–∏–∑–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö, –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Lempel-Ziv –Ω–∞–ø—Ä—è–º—É—é.
        c_lz_approx = max(0, len(compressed)) 
        
        # 3. –≠–Ω—Ç—Ä–æ–ø–∏—è –®–µ–Ω–Ω–æ–Ω–∞
        entropy_h = -spike_density * math.log2(spike_density) - (1 - spike_density) * math.log2(1 - spike_density)
        
        # 4. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–∑–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å —É–∂–µ —á–∞—Å—Ç–∏—á–Ω–æ –µ—Å—Ç—å, –Ω–æ —É—Å–∏–ª–∏–º)
        theoretical_min_bits = L * entropy_h
        
        # –ï—Å–ª–∏ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π –º–∏–Ω–∏–º—É–º –±–∏—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –º–∞–ª (—Å–∏–≥–Ω–∞–ª —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–æ–π), PCI –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –∏–ª–∏ 0
        if theoretical_min_bits < 50.0:  # –ü–æ—Ä–æ–≥ –≤ –±–∏—Ç–∞—Ö
            return 0.0
        
        pci = (c_lz_approx * 8.0) / theoretical_min_bits
        
        return pci
    @staticmethod
    def apply_tms_pulse(layer, strength: float = 20.0):
        n_focal = int(layer.N * 0.15)
        center = torch.randint(0, layer.N, (1,)).item()
        indices = torch.arange(center - n_focal//2, center + n_focal//2) % layer.N
        pulse = torch.zeros(layer.N, device=DEVICE)
        pulse[indices] = strength * (1.0 + torch.randn(len(indices), device=DEVICE) * 0.2)
        layer.I_ext += pulse
        layer.phase[indices] = 0.0

# ==========================================
# 11. –¢–ï–õ–û –ò –ê–ö–¢–ò–í–ù–´–ô –í–´–í–û–î
# ==========================================

class BodyAgent:
    """
    –ê–≥–µ–Ω—Ç –ê–∫—Ç–∏–≤–Ω–æ–≥–æ –í—ã–≤–æ–¥–∞ (Active Inference).
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç:
    1. Proprioceptive Loop (Reflex): –ë—ã—Å—Ç—Ä–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ–∑—ã.
    2. Active Inference: da/dt = -‚àÇF/‚àÇa (–º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Å–µ–Ω—Å–æ—Ä–Ω–æ–π –æ—à–∏–±–∫–∏).
    3. Exploration: –°—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏.
    4. Sense of Agency (SoA) —Å–æ–≥–ª–∞—Å–Ω–æ Section 6 –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏.
    """
    def __init__(self, n_sensors: int, n_actuators: int):
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–µ–ª–∞
        self.position = torch.tensor([0.0], device=DEVICE)
        self.velocity = torch.tensor([0.0], device=DEVICE)
        
        # –°–µ–Ω—Å–æ—Ä–∏–∫–∞
        self.n_sensors = n_sensors
        self.sensory_input = torch.zeros(n_sensors, device=DEVICE)
        
        # –ü—Ä–æ–ø—Ä–∏–æ—Ü–µ–ø—Ü–∏—è
        self.proprioception_real = torch.zeros(n_actuators, device=DEVICE)
        self.proprioception_pred = torch.zeros(n_actuators, device=DEVICE)
        
        # Action Dynamics
        self.action_val = torch.zeros(n_actuators, device=DEVICE) 
        
        # Sense of Agency (—á—É–≤—Å—Ç–≤–æ –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞)
        self.sense_of_agency = 1.0
        
        # Exploration parameters
        self.uncertainty_accumulator = 0.0
        
        # [NEW] Override for Lobotomy/God Mode
        self.action_override = None

    def get_sensory_jacobian(self, sensory_input: torch.Tensor) -> torch.Tensor:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É ‚àÇS/‚àÇa. 
        –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç —Å–µ–Ω—Å–æ—Ä–Ω–æ–≥–æ –ø–æ–ª—è.
        """
        grad = torch.gradient(sensory_input)[0]
        return -grad  

    def update_action(self, dt: float, 
                      sensory_prediction_error: torch.Tensor, 
                      sensory_precision: torch.Tensor,
                      environment_target: float,
                      dopamine_level: float):
        """
        –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –ê–∫—Ç–∏–≤–Ω–æ–≥–æ –í—ã–≤–æ–¥–∞ –∏ —Ä–∞—Å—á–µ—Ç SoA.
        """
        
        # 1. Physics Simulation (Environment)
        self.velocity += (self.action_val - 2.0 * self.velocity) * dt 
        self.position += self.velocity * dt
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–Ω—Å–æ—Ä–Ω–æ–≥–æ –≤—Ö–æ–¥–∞
        true_error = environment_target - self.position
        tuning_curves = torch.linspace(-1, 1, self.n_sensors, device=DEVICE)
        self.sensory_input = torch.exp(-5.0 * ((tuning_curves - true_error) ** 2))
        
        # 2. Proprioceptive Loop (Reflex)
        k_d = 2.0
        reflex_force = -k_d * self.velocity
        
        # 3. Active Inference (Goal-Directed)
        # da/dt = - (Œ†_s ¬∑ Œµ_s) ¬∑ ‚àÇS/‚àÇa
        jacobian = self.get_sensory_jacobian(self.sensory_input)
        # Fix: Cast to float to match Jacobian (float32) for dot product
        weighted_error = sensory_prediction_error.float() * sensory_precision.float()
        fe_gradient = torch.dot(weighted_error, jacobian.float()).unsqueeze(0)
        
        alpha_action = 5.0 * (1.0 + dopamine_level)
        da_goal = -alpha_action * fe_gradient
        
        # 4. Exploration strategy
        current_uncertainty = torch.mean(torch.abs(sensory_prediction_error))
        self.uncertainty_accumulator = 0.95 * self.uncertainty_accumulator + 0.05 * current_uncertainty
        
        beta_explore = torch.tanh(self.uncertainty_accumulator * 2.0)
        
        if dopamine_level < 0.2: 
            beta_explore += 0.5
            
        noise = torch.randn_like(self.action_val, device=DEVICE) * 2.0
        
        # 5. Final Action Integration
        total_force = (1.0 - beta_explore) * da_goal + reflex_force + beta_explore * noise
        self.action_val += total_force * dt
        self.action_val = torch.clamp(self.action_val, -10.0, 10.0)

        # ==========================================
        # 6. Update Sense of Agency (SoA) - FIXED
        # ==========================================
        # Theory Section 6: SoA(t) = exp(-lambda * C_action)
        # C_action = ||outcome_predicted - outcome_actual||^2
        
        # FIX 1: Use Mean Squared Error (MSE) instead of Sum to handle 1000 sensors dimensionality
        mse_error = torch.mean(sensory_prediction_error**2)
        
        # FIX 2: Tuned lambda (sensitivity). 
        # If lambda is too high, any tiny error kills SoA. 
        # lambda=10.0 means 10% MSE error results in SoA ~ 0.36
        lambda_soa = 0.5
        
        target_soa = torch.exp(-lambda_soa * mse_error)
        
        # FIX 3: Temporal Smoothing (Agency implies temporal integration)
        # SoA shouldn't flicker instantly with noise.
        tau_soa = 0.5 # seconds
        alpha_soa = dt / tau_soa
        
        self.sense_of_agency = self.sense_of_agency + alpha_soa * (target_soa - self.sense_of_agency)
        # self.sense_of_agency = max(0.0, min(self.sense_of_agency, 1.0))
        # Keep as tensor? Check usage. 
        # If sense_of_agency is tensor (which it becomes if target_soa is tensor), max/min require torch.clamp
        if isinstance(self.sense_of_agency, torch.Tensor):
             self.sense_of_agency = torch.clamp(self.sense_of_agency, 0.0, 1.0)
        else:
             self.sense_of_agency = max(0.0, min(self.sense_of_agency, 1.0))

    def get_action_vector(self) -> torch.Tensor:
        if self.action_override is not None:
            return self.action_override.to(DEVICE)
        return self.action_val
    
    def activate_mirror_neurons(self, observed_action: torch.Tensor, target_size: int) -> torch.Tensor:
        input_dim = observed_action.shape[0]
        if input_dim == 0:
            return torch.zeros(target_size, device=DEVICE)
        repeats = (target_size // input_dim) + 1
        extended = observed_action.repeat(repeats)[:target_size]
        signal = extended * 0.5
        return signal
    
# ==========================================
# 12. –ú–ï–ù–ï–î–ñ–ï–† –°–ù–ê –ò –ë–ò–û–†–ò–¢–ú–û–í
# ==========================================

class SleepCycleManager:
    def __init__(self, chem: BioChemistry):
        self.chem = chem
        
        self.time_of_day = 0.0 
        self.sleep_pressure = 0.0 
        self.current_stage = 'Wake'
        self.circadian_process = 0.0
        
        self.pressure_buildup_rate = 1.0 / 16.0 
        self.pressure_decay_rate = 1.0 / 8.0    
        
    def update(self, dt_hours: float, current_stress: torch.Tensor):
        self.time_of_day = (self.time_of_day + dt_hours) % 24.0
        
        self.circadian_process = math.sin(2 * math.pi * (self.time_of_day - 8) / 24.0)
        
        if self.current_stage == 'Wake':
            self.sleep_pressure += self.pressure_buildup_rate * dt_hours
        else:
            self.sleep_pressure -= self.pressure_decay_rate * dt_hours
        
        self.sleep_pressure = max(0.0, min(self.sleep_pressure, 2.0))
        
        sleep_drive = self.sleep_pressure - self.circadian_process 
        
        stress_val = current_stress.item() if isinstance(current_stress, torch.Tensor) else current_stress
        
        if self.current_stage == 'Wake':
            if sleep_drive > 1.2: 
                self.current_stage = 'SWS'
        
        elif self.current_stage == 'SWS':
            if self.sleep_pressure < 0.8 and torch.rand(1).item() < 0.05:
                self.current_stage = 'REM'
            elif self.sleep_pressure < 0.1: 
                self.current_stage = 'Wake'
                
        elif self.current_stage == 'REM':
            if torch.rand(1).item() < 0.1: 
                self.current_stage = 'SWS'

        self.apply_stage_effects()

    def apply_stage_effects(self):
        if self.current_stage == 'Wake':
            self.chem.acetylcholine = torch.tensor(0.8, device=DEVICE) 
            self.chem.norepinephrine = torch.tensor(0.8, device=DEVICE)
            self.chem.serotonin = torch.tensor(0.6, device=DEVICE)
        elif self.current_stage == 'SWS':
            self.chem.acetylcholine = torch.tensor(0.1, device=DEVICE) 
            self.chem.norepinephrine = torch.tensor(0.3, device=DEVICE)
        elif self.current_stage == 'REM':
            self.chem.acetylcholine = torch.tensor(0.9, device=DEVICE)
            self.chem.norepinephrine = torch.tensor(0.05, device=DEVICE)
            self.chem.serotonin = torch.tensor(0.05, device=DEVICE)

# ==========================================
# 13. GENOME MANAGER (MTS)
# ==========================================

class GenomeManager:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –≥–µ–Ω–æ–º–∞: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ, –º—É—Ç–∞—Ü–∏—è, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ.
    –†–µ–∞–ª–∏–∑—É–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏—é MTS (Metabolic Topological Sporulation).
    """
    def __init__(self, spore_dir="spores"):
        self.spore_dir = spore_dir
        if not os.path.exists(spore_dir):
            os.makedirs(spore_dir)
            
    def extract(self, simulator) -> dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –≥–µ–Ω–æ–º (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é) –∏–∑ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞."""
        # 1. Chemistry DNA
        chemistry_dna = {
            "tau_mem": float(simulator.phys_cfg.tau_mem),
            "learning_rate": 0.01, 
        }
        
        # 2. Topology DNA
        topology_dna = {
            "type": "Dynamic" if "Neurogenesis" in simulator.__class__.__name__ else "Fixed",
            "layers": [
                {"id": i, "size": level.layer.N} 
                for i, level in enumerate(simulator.hierarchy.levels)
            ]
        }
        
        return {
            "generation": getattr(simulator, "gen", 0),
            "chemistry": chemistry_dna,
            "topology": topology_dna,
            "parent_id": str(id(simulator))
        }

    def mutate(self, genome: dict) -> dict:
        """Applies mutations to the genome."""
        child = json.loads(json.dumps(genome)) # Deep copy
        child["generation"] += 1
        
        # 1. Parametric Drift (80% chance)
        if torch.rand(1).item() < 0.8:
            drift = 1.0 + (torch.rand(1).item() - 0.5) * 0.1 # +/- 5%
            child["chemistry"]["tau_mem"] *= drift
            
        # 2. Synaptic Rewiring (15% chance)
        if torch.rand(1).item() < 0.15:
            child["mutation_log"] = child.get("mutation_log", []) + ["Rewiring"]
            
        return child

    def save(self, genome: dict, filename: str):
        path = os.path.join(self.spore_dir, filename)
        with open(path, 'w') as f:
            json.dump(genome, f, indent=2)
        return path

# ==========================================
# 14. –ì–õ–ê–í–ù–´–ô –°–ò–ú–£–õ–Ø–¢–û–†
# ==========================================

class ConsciousnessSimulator:
    def __init__(self, use_small_world: bool = True):
        print("Initializing Mathematical Model of Consciousness")
        print(f"Topology: {'Small-World' if use_small_world else 'Random'}")
        
        self.phys_cfg = PhysicsConfig()
        self.chem_cfg = ChemistryConfig()
        self.topo_cfg = TopologyConfig() if use_small_world else None
        
        self.chemistry = BioChemistry(self.chem_cfg)
        self.sleep_manager = SleepCycleManager(self.chemistry)
        self.genome_manager = GenomeManager()
        self.gen = 0

        self.hippocampus = HippocampalSystem()
        
        # Increase input dimension for specialized specialized text/visual embedding
        self.hierarchy = BrainHierarchy(self.phys_cfg, self.chemistry, self.topo_cfg, input_dim=2000)
        self.gwt = GlobalWorkspace(self.hierarchy)
        
        self.affect = AffectiveSystem(self.chemistry)
        self.social = SocialCognition()
        
        self.body = BodyAgent(n_sensors=2000, n_actuators=1) 
        
        self.environment_target = 0.5
        self.simulation_time = 0.0
        self.has_reproduced = False

    def inject_pathology(self, condition: str):
        """
        Injects pathological states into the system.
        Conditions: 'Anesthesia_Propofol', 'Depression', 'Social_Anxiety'
        """
        print(f"!!! INJECTING PATHOLOGY: {condition} !!!")
        
        if condition == "Anesthesia_Propofol":
            self.chemistry.propofol_conc = torch.tensor(3.0, device=DEVICE) 
            
        elif condition == "Depression":
            self.chemistry.dopamine = torch.tensor(0.1, device=DEVICE)
            self.chemistry.serotonin = torch.tensor(0.1, device=DEVICE)
            self.affect.valence = torch.tensor(-0.8, device=DEVICE)
            
        elif condition == "Social_Anxiety":
            # Initialize m2 if not already present or modify existing
            self.social.m2_self_in_other = torch.ones(10, device=DEVICE) * -1.0 
            self.affect.amygdala_activity = torch.tensor(0.9, device=DEVICE)

            self.affect.amygdala_activity = torch.tensor(0.9, device=DEVICE)

    def biological_lifecycle(self):
        """
        –†–µ–∞–ª–∏–∑—É–µ—Ç MTS (Metabolic Topological Sporulation).
        –¢—Ä–∏–≥–≥–µ—Ä —Ä–∞–∑–º–Ω–æ–∂–µ–Ω–∏—è: SWS + High ATP + Low Stress.
        """
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π —Ä–∞–∑–º–Ω–æ–∂–µ–Ω–∏—è (–≤–æ –≤—Ä–µ–º—è —Å–Ω–∞)
        if self.sleep_manager.current_stage == 'SWS':
            
            # –≠–Ω–µ—Ä–≥–∏—è (average across all layers being > 0.95 means huge surplus)
            total_atp = 0.0
            for level in self.hierarchy.levels:
                total_atp += level.layer.ATP.mean().item()
            avg_atp = total_atp / len(self.hierarchy.levels)
            
            # –°—Ç—Ä–µ—Å—Å (low norepinephrine)
            stress_val = self.chemistry.norepinephrine.item()
            
            if avg_atp > 0.95 and stress_val < 0.4:
                # === –ê–ö–¢ –†–ê–ó–ú–ù–û–ñ–ï–ù–ò–Ø ===
                print(f"üß¨ SPORULATION EVENT INITIATED (ATP={avg_atp:.2f}, Stress={stress_val:.2f})")
                
                try:
                    # –ê. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –î–ù–ö
                    parent_dna = self.genome_manager.extract(self)
                    
                    # –ë. –ú—É—Ç–∞—Ü–∏—è (NEAT)
                    child_dna = self.genome_manager.mutate(parent_dna)
                    
                    # –í. –í—ã–±—Ä–æ—Å —Å–ø–æ—Ä—ã (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON)
                    filename = f"gen_{self.gen + 1}_child_{id(self)}_{int(self.simulation_time)}.json"
                    saved_path = self.genome_manager.save(child_dna, filename)
                    
                    # –ì. –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è –ø–ª–∞—Ç–∞ (–ñ–µ—Ä—Ç–≤–∞)
                    for level in self.hierarchy.levels:
                        level.layer.ATP.fill_(0.1) # Drain to 10%
                    
                    self.chemistry.dopamine.fill_(0.0)
                    
                    print(f"‚úÖ Spore saved to {saved_path}. Parent exhausted (ATP -> 0.1).")
                    self.has_reproduced = True
                    
                except Exception as e:
                    print(f"‚ùå Sporulation Failed: {e}")

    @torch.no_grad()
    def step(self, dt: float, time_scale: float = 1.0):
        self.simulation_time += dt
        
        # === –•–†–û–ù–û-–£–°–ö–û–†–ò–¢–ï–õ–¨ ===
        # –ú—ã —É–º–Ω–æ–∂–∞–µ–º dt –Ω–∞ time_scale —Ç–æ–ª—å–∫–æ –¥–ª—è SleepManager.
        # –î–ª—è –Ω–µ–π—Ä–æ–Ω–æ–≤ dt –æ—Å—Ç–∞–µ—Ç—Å—è –º–∞–ª–µ–Ω—å–∫–∏–º (0.05), —á—Ç–æ–±—ã –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –Ω–µ –ª–æ–º–∞–ª–∞—Å—å.
        scaled_dt_hours = (dt * time_scale) / 3600.0
        
        self.sleep_manager.update(dt_hours=scaled_dt_hours, 
                                current_stress=self.affect.arousal)

        # 1. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        action_vec = self.body.get_action_vector()

        # 2. –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç V1 –¥–ª—è —Ç–µ–ª–∞ (Active Inference loop)
        v1_error = self.hierarchy.get_sensory_prediction_error()
        v1_precision = self.hierarchy.levels[0].precision.mean() # Scalar approximation
        
        # 3. –û–±–Ω–æ–≤–ª—è–µ–º –¢–µ–ª–æ –∏ –°—Ä–µ–¥—É (Sensory + Action update)
        if self.sleep_manager.current_stage == 'SWS':
            # –í SWS —Å–µ–Ω—Å–æ—Ä–∏–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞
            self.body.sensory_input.fill_(0.0)
        elif self.sleep_manager.current_stage == 'REM':
            # –í REM —Å–µ–Ω—Å–æ—Ä–∏–∫–∞ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∞—è (—Å–Ω—ã)
            self.body.sensory_input = torch.normal(0, 0.5, size=(self.body.n_sensors,))
        else:
            # Wake: Active Inference
            self.body.update_action(
                dt=dt,
                sensory_prediction_error=v1_error,
                sensory_precision=v1_precision,
                environment_target=self.environment_target,
                dopamine_level=self.chemistry.dopamine.item()
            )
        
        sensory_input = self.body.sensory_input
        
        # 2. Get Global Context (Echo from previous step)
        global_feedback = self.gwt.get_context_feedback()
        
        # 3. Update BioChemistry
        total_free_energy = self.hierarchy.get_global_free_energy()
        self.chemistry.update(dt, 
                            stress_level=self.affect.arousal,
                            reward_prediction_error=0.0)
        
        # 4. Hippocampal Operations
        if self.sleep_manager.current_stage == 'SWS':
            self.hippocampus.replay_sws(self.hierarchy.levels[-1].layer, dt)
        else:
            if hasattr(self, 'gwt'):
                self.hippocampus.store(self.gwt.broadcast_signal, self.affect.valence.item())

        # 5. Process Hierarchy with Global Feedback AND Action Context
        self.hierarchy.process_sensory_input(
            sensory_input=sensory_input,
            action_vector=action_vec,    # <-- –ü–µ—Ä–µ–¥–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            dt=dt, 
            sleep_stage=self.sleep_manager.current_stage,
            global_context=global_feedback
        )
        
        # 6. Global Workspace Step
        self.gwt.step(dt, social_module=self.social)
        
        # 7. Social Cognition Update
        self.social.update_self(action_vec, self.affect.valence.item(), self.affect.arousal.item())
        self.social.recursive_update(dt)
        
        # 8. MTS Biological Lifecycle (Sporulation Check)
        self.biological_lifecycle()

    def save_brain(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤—Å–µ–≥–æ –º–æ–∑–≥–∞ –≤ —Ñ–∞–π–ª"""
        print(f"Saving brain state to {path}...")
        state = {
            'hierarchy': self.hierarchy.get_state(),
            'chemistry': self.chemistry.get_state(),
            'affect': self.affect.get_state(),
            'gwt': self.gwt.get_state(),
            'social': self.social.get_state(),
            'simulation_time': self.simulation_time
        }
        torch.save(state, path)
        print("Brain saved successfully.")

    def load_brain(self, path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–∑–≥–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(path):
            print(f"Brain file {path} not found. Starting fresh.")
            return
            
        print(f"Loading brain state from {path}...")
        try:
            state = torch.load(path, map_location=DEVICE)
            
            if 'hierarchy' in state: self.hierarchy.load_state(state['hierarchy'])
            if 'chemistry' in state: self.chemistry.load_state(state['chemistry'])
            if 'affect' in state: self.affect.load_state(state['affect'])
            if 'gwt' in state: self.gwt.load_state(state['gwt'])
            if 'social' in state: self.social.load_state(state['social'])
            if 'simulation_time' in state: self.simulation_time = state['simulation_time']
            
            print("Brain loaded successfully.")
        except Exception as e:
            print(f"Error loading brain: {e}")

        # 11. Safety Checks (Periodic)
        if self.simulation_time % 10.0 < dt:
            for level in self.hierarchy.levels:
                if not level.layer.validate_state():
                    print(f"‚ùå Critical Error at T={self.simulation_time:.2f}s")
                    
        # 12. –ñ–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª (MTS) - –í—ã–∑—ã–≤–∞–µ–º –≤ –∫–æ–Ω—Ü–µ, —á—Ç–æ–±—ã —ç—Ñ—Ñ–µ–∫—Ç exhaustion —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è
        self.biological_lifecycle()

    def print_status(self):
        stage = self.sleep_manager.current_stage
        phi = self.gwt.phi_current
        n_coalitions = len(self.gwt.active_coalitions)
        val = self.affect.valence.item()
        arous = self.affect.arousal.item()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        action_display = self.body.get_action_vector().mean().item()
        
        print(f"T={self.simulation_time:.2f}s | Stg: {stage} | "
              f"Phi: {phi:.3f} | Coalitions: {n_coalitions} | "
              f"Emo: V={val:.2f}/A={arous:.2f} | "
              f"Action: {action_display:.2f}")

class SimulatorWithNeurogenesis(ConsciousnessSimulator):
    def __init__(self, use_small_world: bool = True):
        print("Initializing Dynamic Neurogenesis Model")
        
        self.phys_cfg = PhysicsConfig()
        self.chem_cfg = ChemistryConfig()
        self.topo_cfg = TopologyConfig() if use_small_world else None
        
        # --- CONFIGURATION OF GROWTH ---
        self.neuro_cfg = NeurogenesisConfig()
        
        self.chemistry = BioChemistry(self.chem_cfg)
        self.sleep_manager = SleepCycleManager(self.chemistry)
        self.hippocampus = HippocampalSystem()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –î–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –ò–µ—Ä–∞—Ä—Ö–∏—é
        self.hierarchy = DynamicBrainHierarchy(self.phys_cfg, self.chemistry, self.topo_cfg, self.neuro_cfg)
        
        # –ú–µ–Ω–µ–¥–∂–µ—Ä –ù–µ–π—Ä–æ–≥–µ–Ω–µ–∑–∞
        self.gardener = NeurogenesisManager(self.hierarchy, self.neuro_cfg)
        self.genome_manager = GenomeManager() # <--- Added missing manager
        
        self.gwt = GlobalWorkspace(self.hierarchy)
        self.affect = AffectiveSystem(self.chemistry)
        self.social = SocialCognition()
        self.body = BodyAgent(n_sensors=2000, n_actuators=1) 
        
        self.environment_target = 0.5
        self.simulation_time = 0.0
        self.has_reproduced = False

    def step(self, dt: float, time_scale: float = 1.0):
        # 1. –û–±—ã—á–Ω—ã–π —à–∞–≥ —Ñ–∏–∑–∏–∫–∏ (–≤—ã–∑–æ–≤–µ—Ç super().step(dt) –µ—Å–ª–∏ –±—ã —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–ª–∞—Å—å –∏–¥–µ–∞–ª—å–Ω–æ, 
        # –Ω–æ –º—ã –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –∏–µ—Ä–∞—Ä—Ö–∏—é, —Ç–∞–∫ —á—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º –ª–æ–≥–∏–∫—É step)
        
        # ... (–ö–æ–¥ —à–∞–≥–∞ –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω –±–∞–∑–æ–≤–æ–º—É –∫–ª–∞—Å—Å—É) ...
        # –í—Å—Ç–∞–≤–ª—è–µ–º –≤—ã–∑–æ–≤ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Ä–æ—Å—Ç–∞ –ü–ï–†–ï–î —Ñ–∏–∑–∏–∫–æ–π –∏–ª–∏ –ü–û–°–õ–ï
        
        current_fe = self.hierarchy.get_global_free_energy()
        
        # –í—ã–∑—ã–≤–∞–µ–º –°–∞–¥–æ–≤–Ω–∏–∫–∞ (Gardener) –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Å–ª–æ–≤–∏–π —Ä–æ—Å—Ç–∞
        self.gardener.update(current_fe)
        
        # –î–∞–ª–µ–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ü–∏–∫–ª:
        super().step(dt, time_scale=time_scale)
        
    def print_status(self):
        super().print_status()
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ –æ —Ä–∞–∑–º–µ—Ä–µ –º–æ–∑–≥–∞
        sizes = [u.layer.N for u in self.hierarchy.levels]
        print(f"   üß¨ Brain Size: {sizes} | Phase: {self.gardener.phase}")

# ==========================================
# 15. –í–ò–ó–£–ê–õ–¨–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø (Unified)
# ==========================================

def run_demo_with_plots(
    dt: float = 0.05,
    total_steps: int = 800,
    anesthesia_step: int = 400,
    experiment_mode: str = "ANESTHESIA",
    psychiatry_mode: Optional[str] = None,
    warmup_steps: int = 20,
    use_small_world: bool = True
):
    sim = ConsciousnessSimulator(use_small_world=use_small_world)

    print("=== NeuralBiocore Unified: Simulation Demo ===")
    print(f"Topology: {'Small-World' if use_small_world else 'Random'}")
    print(f"Mode: {experiment_mode}, Steps: {total_steps}, dt: {dt}s")

    # --- WARMUP PROGRESS BAR ---
    # leave=False –∑–∞—Å—Ç–∞–≤–∏—Ç –±–∞—Ä –∏—Å—á–µ–∑–Ω—É—Ç—å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –≤—ã–≤–æ–¥
    for _ in tqdm(range(warmup_steps), desc="üî• Warming up", leave=False, ncols=100):
        sim.step(dt)
    
    print(f"Warmup complete. T={sim.simulation_time:.2f}s")

    # Pathologies
    if psychiatry_mode == "DEPRESSION":
        sim.inject_pathology("Depression")
    elif psychiatry_mode == "MANIA":
        sim.chemistry.dopamine = torch.tensor(0.9, device=DEVICE)      
        sim.chemistry.norepinephrine = torch.tensor(0.8, device=DEVICE) 
        sim.affect.valence = torch.tensor(0.8, device=DEVICE)          

    # Data Collection
    times = []
    phi_values = []
    valences = []
    arousals = []
    propofol_levels = []
    v1_activity = []
    v1_sync = []
    atp_levels = []      
    soa_values = []      
    dead_neurons = []    
    sleep_stages_history = []

    # --- MAIN LOOP PROGRESS BAR ---
    for step in tqdm(range(total_steps), desc=f"üß† Simulating ({experiment_mode})", ncols=100):
        current_time = sim.simulation_time

        # Scenario: Anesthesia
        if experiment_mode == "ANESTHESIA":
            if step >= anesthesia_step:
                sim.chemistry.propofol_conc += 0.02 
                if sim.chemistry.propofol_conc > 5.0:
                    sim.chemistry.propofol_conc = torch.tensor(5.0, device=DEVICE)

        # Scenario: Awakening
        elif experiment_mode == "AWAKENING":
            if 15.0 <= current_time < 45.0:
                sim.chemistry.propofol_conc += 0.02
                if sim.chemistry.propofol_conc > 4.0: sim.chemistry.propofol_conc = torch.tensor(4.0, device=DEVICE)
            elif current_time >= 45.0:
                sim.chemistry.propofol_conc *= 0.95

        # Scenario: Dreams
        elif experiment_mode == "REM_DREAMS":
            if 10.0 <= sim.simulation_time < 30.0:
                sim.sleep_manager.current_stage = 'REM'
            elif 30.0 <= sim.simulation_time < 40.0:
                 sim.sleep_manager.current_stage = 'Wake'

        sim.step(dt)

        # Record Metrics
        times.append(sim.simulation_time)
        phi_values.append(float(sim.gwt.phi_current))
        valences.append(sim.affect.valence.item())
        arousals.append(sim.affect.arousal.item())
        propofol_levels.append(sim.chemistry.propofol_conc.item())
        sleep_stages_history.append(sim.sleep_manager.current_stage)

        v1_layer = sim.hierarchy.levels[0].layer
        v1_activity.append(v1_layer.get_activity_rate().item())
        R, _ = v1_layer.compute_kuramoto_order()
        v1_sync.append(R.item())
        
        atp_levels.append(v1_layer.ATP.mean().item())
        dead_neurons.append(v1_layer.is_dead.sum().item())
        val_soa = sim.body.sense_of_agency
        if isinstance(val_soa, torch.Tensor):
            val_soa = val_soa.item()
        soa_values.append(val_soa)

    # Plotting (–∫–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ—Å—Ç–∞–ª—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    times_np = np.array(times)
    fig, axes = plt.subplots(5, 1, figsize=(12, 14), sharex=True)

    # 1. Consciousness & Propofol
    ax = axes[0]
    ax.plot(times_np, phi_values, label="Œ¶ (Integrated Info)", color="tab:blue", linewidth=2)
    ax.set_ylabel("Œ¶ / Consciousness")
    ax2 = ax.twinx()
    ax2.plot(times_np, propofol_levels, label="[Propofol]", color="tab:red", linestyle="--")
    ax2.set_ylabel("[Propofol] ¬µM")
    ax.legend(loc="upper left")
    ax2.legend(loc="upper right")
    ax.set_title(f"Simulation Mode: {experiment_mode} (Small-World: {use_small_world})")

    # 2. Emotions
    ax = axes[1]
    ax.plot(times_np, valences, label="Valence (Pleasure)", color="tab:green")
    ax.plot(times_np, arousals, label="Arousal (Energy)", color="tab:orange")
    ax.set_ylabel("Affect")
    ax.legend(loc="upper right")
    ax.grid(True, alpha=0.3)

    # 3. Energy & Death
    ax = axes[2]
    ax.plot(times_np, atp_levels, label="ATP Level (V1 Mean)", color="gold", linewidth=2)
    ax.set_ylabel("Metabolic Energy")
    ax.set_ylim(0, 1.1)
    if max(dead_neurons) > 0:
        ax3 = ax.twinx()
        ax3.plot(times_np, dead_neurons, label="Dead Neurons", color="black", linestyle=":")
        ax3.set_ylabel("Count Dead")
        ax3.legend(loc="lower right")
    ax.legend(loc="upper right")
    ax.grid(True, alpha=0.3)

    # 4. Sense of Agency
    ax = axes[3]
    ax.plot(times_np, soa_values, label="Sense of Agency", color="purple")
    ax.set_ylabel("SoA (0-1)")
    ax.set_ylim(-0.1, 1.1)
    ax.grid(True, alpha=0.3)
    ax.legend(loc="upper right")

    # 5. Neural Dynamics
    ax = axes[4]
    ax.plot(times_np, v1_activity, label="Firing Rate", color="tab:purple")
    ax.plot(times_np, v1_sync, label="Synchronization (R)", color="tab:brown")
    ax.set_ylabel("Neural Dynamics")
    ax.set_xlabel("Time (s)")
    ax.legend(loc="upper right")

    # Sleep Stage Backgrounds
    stage_colors = {"Wake": "white", "SWS": "#e6e6fa", "REM": "#ffe4e1"}
    current_st = sleep_stages_history[0]
    start_t = times_np[0]
    for i, st in enumerate(sleep_stages_history):
        if st != current_st:
            for ax_i in axes:
                ax_i.axvspan(start_t, times_np[i], color=stage_colors.get(current_st, "white"), alpha=0.5)
            current_st = st
            start_t = times_np[i]
    for ax_i in axes:
        ax_i.axvspan(start_t, times_np[-1], color=stage_colors.get(current_st, "white"), alpha=0.5)

    plt.tight_layout()
    plt.show()


# ==========================================
# 15. LIFECYCLE DEMO
# ==========================================

def run_sleep_lifecycle_demo(
    total_hours: float = 24.0,
    dt_minutes: float = 5.0,
):
    print("=== Sleep/Wake Cycle Demo (Unified) ===")
    
    chem_cfg = ChemistryConfig()
    chemistry = BioChemistry(chem_cfg)
    sleep_manager = SleepCycleManager(chemistry)

    n_steps = int(total_hours * 60.0 / dt_minutes)
    dt_hours = dt_minutes / 60.0

    times_h = []
    stages = []
    sleep_pressures = []
    circadian_values = []
    ach_levels = []
    ne_levels = []
    serotonins = []

    # --- LIFECYCLE PROGRESS BAR ---
    for _ in tqdm(range(n_steps), desc="üåô Cycling Circadian Rhythms", ncols=100):
        current_hour = sleep_manager.time_of_day
        stress = 0.3 if 8 <= current_hour <= 20 else 0.05
        
        sleep_manager.update(dt_hours=dt_hours, current_stress=torch.tensor(stress, device=DEVICE))

        times_h.append(sleep_manager.time_of_day)
        stages.append(sleep_manager.current_stage)
        sleep_pressures.append(sleep_manager.sleep_pressure)
        circadian_values.append(sleep_manager.circadian_process)
        ach_levels.append(chemistry.acetylcholine.item())
        ne_levels.append(chemistry.norepinephrine.item())
        serotonins.append(chemistry.serotonin.item())

    # Plotting
    times_h_np = np.array(times_h)
    
    stage_map = {"Wake": 2, "REM": 1, "SWS": 0}
    stage_numeric = np.array([stage_map.get(s, np.nan) for s in stages])

    fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)

    ax = axes[0]
    ax.step(times_h_np, stage_numeric, where="post", color="tab:blue")
    ax.set_yticks([0, 1, 2])
    ax.set_yticklabels(["SWS", "REM", "Wake"])
    ax.set_title("Circadian Lifecycle")
    ax.grid(True, alpha=0.3)

    ax = axes[1]
    ax.plot(times_h_np, sleep_pressures, label="Sleep pressure S(t)", color="tab:red")
    ax.plot(times_h_np, circadian_values, label="Circadian process C(t)", color="tab:green")
    ax.legend(loc="upper right")
    ax.grid(True, alpha=0.3)

    ax = axes[2]
    ax.plot(times_h_np, ach_levels, label="ACh", color="tab:orange")
    ax.plot(times_h_np, ne_levels, label="NE", color="tab:purple")
    ax.plot(times_h_np, serotonins, label="5-HT", color="tab:brown")
    ax.set_xlabel("Hours")
    ax.legend(loc="upper right")
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# ==========================================
# 16. PCI EXPERIMENT (Hardened Physics)
# ==========================================

def run_pci_experiment(dt: float = 0.001, use_small_world: bool = True):
    print(f"=== PCI Experiment: Fast Optimized (Small-World: {use_small_world}) ===")
    
    conditions = [
        ("Wake", 0.4, 0.0, 0.8, 0.8),
        ("Light_Anesthesia", 0.15, 2.0, 0.4, 0.4),
        ("Deep_Anesthesia", 0.02, 5.0, 0.1, 0.1)
    ]
    
    results = {}
    
    for condition_name, coupling, propofol, ach, ne in tqdm(conditions, desc="üî¨ Total Progress", ncols=100):
        tqdm.write(f"\n--- Condition: {condition_name} ---")
        
        sim = ConsciousnessSimulator(use_small_world=use_small_world)
        sim.phys_cfg.coupling_strength = coupling 
        sim.phys_cfg.alpha_sync = 0.3              
        sim.chemistry.propofol_conc = torch.tensor(propofol, device=DEVICE)
        sim.chemistry.acetylcholine = torch.tensor(ach, device=DEVICE) 
        sim.chemistry.norepinephrine = torch.tensor(ne, device=DEVICE)
        
        if propofol > 0:
            for lvl in sim.hierarchy.levels:
                lvl.layer.p_cfg.v_threshold *= (1.0 + 0.1 * propofol)

        # 1. Stabilizing
        for _ in tqdm(range(500), desc="  üîÑ Stabilizing", leave=True, ncols=80):
            noise = 1.0 if condition_name == "Wake" else 0.2
            for unit in sim.hierarchy.levels:
                unit.layer.I_ext += torch.randn(unit.layer.N) * noise
            sim.step(dt)
        
        # 2. TMS Pulse
        target_layer = sim.hierarchy.levels[2].layer 
        tqdm.write(f"  ‚ö° Pulse -> {target_layer.id}")
        PCICalculator.apply_tms_pulse(target_layer, strength=60.0)
        
        # 3. Recording
        recording_window_ms = 400
        steps = int(recording_window_ms / (dt * 1000))
        all_spikes = []
        
        for t in tqdm(range(steps), desc="  üìâ Recording", leave=True, ncols=80):
            bg_noise = 0.1
            for unit in sim.hierarchy.levels:
                unit.layer.I_ext += torch.randn(unit.layer.N) * bg_noise
            sim.step(dt)
            combined_spikes = torch.cat([u.layer.spikes for u in sim.hierarchy.levels])
            all_spikes.append(combined_spikes)
            
        # 4. PCI Calculation
        spike_matrix = torch.stack(all_spikes)
        analysis_matrix = spike_matrix[5:, :] if spike_matrix.shape[0] > 10 else spike_matrix
        
        pci = PCICalculator.compute_pci(analysis_matrix)
        
        total_spikes = analysis_matrix.sum().item()
        active_neurons = (analysis_matrix.sum(dim=0) > 0).sum().item()
        
        tqdm.write(f"  üìä PCI: {pci:.4f} | Spikes: {int(total_spikes)}")
        
        results[condition_name] = {
            'pci': pci,
            'spike_count': int(total_spikes),
            'active_neurons': active_neurons
        }

    print("\n=== FINAL RESULTS ===")
    print(f"{'Condition':<20} {'PCI':>8} {'Spikes':>10}")
    print("-" * 40)
    for c, metrics in results.items():
        print(f"{c:<20} {metrics['pci']:>8.4f} {metrics['spike_count']:>10}")

if __name__ == "__main__":
    # Uncomment the function you wish to run:
    
    # 1. Visual Simulation (Wake/Anesthesia) with Small-World Topology
    run_demo_with_plots(use_small_world=True, experiment_mode="ANESTHESIA")
    
    # 2. Sleep Cycle Analysis
    # run_sleep_lifecycle_demo()
    
    # 3. PCI Complexity Experiment
    # run_pci_experiment(use_small_world=True)